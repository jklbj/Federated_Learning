{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages',\n",
       " '/home/mnetlab/anaconda3/envs/pp_fl/lib/python36.zip',\n",
       " '/home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6',\n",
       " '/home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/mnetlab/.local/lib/python3.6/site-packages',\n",
       " '/home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages',\n",
       " '/home/mnetlab/.local/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/mnetlab/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Modified fedavg for our input\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Other import\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from data.preprocessing import preprocessing_for_training, separate_and_preprocess_for_simple_fed, evaluate_with_new_model\n",
    "from data.read_data import read_data, read_setting\n",
    "from data.data_utils import load_cifar10_data, train_test_label_to_categorical\n",
    "from model.model import init_model, record_history, training_once, print_result_for_fed\n",
    "from model.operation import broadcast_to_device, caculate_delta, aggregate_add, aggregate_division_return \n",
    "from config.environment import gpu_decision, zmq_bind\n",
    "\n",
    "from math import floor\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import multiprocessing as mp\n",
    "import zmq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    epoch = _ + epo\n",
    "    # initial_lrate = 1.0 # no longer needed\n",
    "    drop = 0.99\n",
    "    epochs_drop = 1.0\n",
    "    \n",
    "    lrate = 0.2 * pow(drop, floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, epo = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(r, device):\n",
    "    # Load CIFAR-10 data\n",
    "    train_images, train_labels, test_images, test_labels = load_cifar10_data()\n",
    "    \n",
    "    gpu_decision(training_info[\"num_gpu\"] ,device)\n",
    "            \n",
    "    model_m = tf.keras.models.load_model('parallel/model_m.h5')\n",
    "\n",
    "    # Define our model\n",
    "    if(r == 0):\n",
    "        locals()['model_{}'.format(device)] = init_model()\n",
    "    else:\n",
    "        locals()['model_{}'.format(device)] = tf.keras.models.load_model('parallel/model_m.h5')\n",
    "    \n",
    "    # Local training on each device\n",
    "    for epo in range(training_info[\"num_local_epoch\"]):\n",
    "        train_new_image, train_new_label = separate_and_preprocess_for_simple_fed(device, train_images, train_labels, training_info[\"num_device\"])\n",
    "        history_temp = training_once(locals()['model_{}'.format(device)], train_new_image, train_new_label, training_info, augment, callback)\n",
    "\n",
    "    # Calculate delta weight on each device\n",
    "    caculate_delta(locals()['model_{}'.format(device)], model_m)\n",
    "        \n",
    "    # Save delta weight in each model weight(.h5) file \n",
    "    try:\n",
    "        if(r == 0):\n",
    "            locals()['model_{}'.format(device)].save('parallel/model_{}.h5'.format(device))\n",
    "        else:\n",
    "            locals()['model_{}'.format(device)].save_weights('parallel/model_{}.h5'.format(device))\n",
    "    except:\n",
    "        print(\"error found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Received) round: b'0:0', device: 0\n",
      "(Received) round: b'0:1', device: 1\n",
      "(Received) round: b'0:2', device: 2\n",
      "(Received) round: b'0:3', device: 3\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 5:38 - loss: 3.9940 - acc: 0.160 - ETA: 23s - loss: 3.3056 - acc: 0.1271  - ETA: 5:47 - loss: 3.8887 - acc: 0.1200 - ETA: 5:48 - loss: 3.7018 - acc: 0.0 - ETA: 16s - loss: 2.9717 - acc: 0.1543 - ETA: 28s - loss: 3.6371 - acc: 0.1517  - ETA: 5:52 - loss: 4.3177 - acc: 0.020 - ETA: 38s - loss: 3.6899 - acc: 0.122 - ETA: 10s - loss: 2.7501 - acc: 0.16 - ETA: 15s - loss: 3.0569 - acc: 0.159 - ETA: 35s - loss: 3.6411 - acc: 0.126 - ETA: 19s - loss: 3.0932 - acc: 0.14 - ETA: 8s - loss: 2.6264 - acc: 0.176 - ETA: 10s - loss: 2.8360 - acc: 0.16 - ETA: 18s - loss: 3.0742 - acc: 0.13 - ETA: 12s - loss: 2.8582 - acc: 0.152 - ETA: 6s - loss: 2.5393 - acc: 0.1 41/250 [===>..........................] - ETA: 11s - loss: 2.7999 - acc: 0.1490 - ETA: 8s - loss: 2.7029 - acc: 0.168 - ETA: 8s - loss: 2.6873 - acc: 0.1595 - ETA: 5s - loss: 2.4841 - acc: 0.190 - ETA: 6s - loss: 2.6083 - acc: 0.17 - ETA: 8s - loss: 2.6830 - acc: 0.1560 - ETA: 7s - loss: 2.6164 - acc: 0.167 - ETA: 4s - loss: 2.4479 - acc: 0.19 - ETA: 7s - loss: 2.6005 - acc: 0.1654 - ETA: 5s - loss: 2.5266 - acc: 0.186 - ETA: 6s - loss: 2.5575 - acc: 0.170 - ETA: 4s - loss: 2.4215 - acc: 0.19 - ETA: 5s - loss: 2.5448 - acc: 0.1639 - ETA: 4s - loss: 2.4839 - acc: 0.189 - ETA: 5s - loss: 2.5162 - acc: 0.174 - ETA: 3s - loss: 2.3957 - acc: 0.194 - ETA: 5s - loss: 2.4927 - acc: 0.168 - ETA: 4s - loss: 2.4558 - acc: 0.191 - ETA: 4s - loss: 2.4759 - acc: 0.180 - ETA: 3s - loss: 2.3664 - acc: 0.195 - ETA: 4s - loss: 2.4609 - acc: 0.172 - ETA: 3s - loss: 2.4283 - acc: 0.194 - ETA: 3s - loss: 2.4415 - acc: 0.181 - ETA: 2s - loss: 2.3410 - acc: 0.19 - ETA: 3s - loss: 2.4237 - acc: 0.1765 - ETA: 3s - loss: 2.4001 - acc: 0.193 - ETA: 3s - loss: 2.4086 - acc: 0.182 - ETA: 2s - loss: 2.3218 - acc: 0.202 - ETA: 3s - loss: 2.3979 - acc: 0.178 - ETA: 2s - loss: 2.3695 - acc: 0.195 - ETA: 3s - loss: 2.3856 - acc: 0.186 - ETA: 2s - loss: 2.3058 - acc: 0.205 - ETA: 3s - loss: 2.3743 - acc: 0.182 - ETA: 2s - loss: 2.3530 - acc: 0.197 - ETA: 2s - loss: 2.3617 - acc: 0.189 - ETA: 2s - loss: 2.2866 - acc: 0.209 - ETA: 2s - loss: 2.3513 - acc: 0.187 - ETA: 2s - loss: 2.3363 - acc: 0.198 - ETA: 2s - loss: 2.3414 - acc: 0.191 - ETA: 1s - loss: 2.2696 - acc: 0.213 - ETA: 2s - loss: 2.3278 - acc: 0.191 - ETA: 2s - loss: 2.3255 - acc: 0.199 - ETA: 2s - loss: 2.3211 - acc: 0.195 - ETA: 1s - loss: 2.2582 - acc: 0.215 - ETA: 2s - loss: 2.3141 - acc: 0.193 - ETA: 1s - loss: 2.3115 - acc: 0.201 - ETA: 2s - loss: 2.3046 - acc: 0.199 - ETA: 1s - loss: 2.2518 - acc: 0.214 - ETA: 1s - loss: 2.3026 - acc: 0.195141/250 [===============>..............]138/250 [===============>..............] - ETA: 1s - loss: 2.2881 - acc: 0.204 - ETA: 1s - loss: 2.2419 - acc: 0.214 - ETA: 1s - loss: 2.2852 - acc: 0.198 - ETA: 1s - loss: 2.2848 - acc: 0.207 - ETA: 1s - loss: 2.2747 - acc: 0.204 - ETA: 1s - loss: 2.2309 - acc: 0.216 - ETA: 1s - loss: 2.2748 - acc: 0.200 - ETA: 1s - loss: 2.2726 - acc: 0.210 - ETA: 1s - loss: 2.2649 - acc: 0.206 - ETA: 1s - loss: 2.2250 - acc: 0.218165/250 [==================>...........] - ETA: 1s - loss: 2.2628 - acc: 0.2018 - ETA: 1s - loss: 2.2613 - acc: 0.212 - ETA: 1s - loss: 2.2543 - acc: 0.208 - ETA: 0s - loss: 2.2170 - acc: 0.219166/250 [==================>...........] - ETA: 1s - loss: 2.2509 - acc: 0.2150 - ETA: 1s - loss: 2.2528 - acc: 0.203 - ETA: 1s - loss: 2.2465 - acc: 0.209 - ETA: 0s - loss: 2.2096 - acc: 0.220 - ETA: 0s - loss: 2.2417 - acc: 0.215 - ETA: 1s - loss: 2.2433 - acc: 0.205 - ETA: 0s - loss: 2.2379 - acc: 0.212 - ETA: 0s - loss: 2.2033 - acc: 0.22 - ETA: 0s - loss: 2.2321 - acc: 0.2161 - ETA: 0s - loss: 2.2339 - acc: 0.208 - ETA: 0s - loss: 2.2269 - acc: 0.214 - ETA: 0s - loss: 2.1967 - acc: 0.22 - ETA: 0s - loss: 2.2255 - acc: 0.209195/250 [======================>.......] - ETA: 0s - loss: 2.2182 - acc: 0.217 - ETA: 0s - loss: 2.1905 - acc: 0.22203/250 [=======================>......]207/250 [=======================>......] - ETA: 0s - loss: 2.2176 - acc: 0.2120 - ETA: 0s - loss: 2.2188 - acc: 0.218 - ETA: 0s - loss: 2.1810 - acc: 0.225 - ETA: 0s - loss: 2.2099 - acc: 0.212 - ETA: 0s - loss: 2.2107 - acc: 0.219 - ETA: 0s - loss: 2.2018 - acc: 0.220 - ETA: 0s - loss: 2.1763 - acc: 0.22 - ETA: 0s - loss: 2.2019 - acc: 0.2131 - ETA: 0s - loss: 2.2009 - acc: 0.2215220/250 [=========================>....] - ETA: 0s - loss: 2.1928 - acc: 0.223 - ETA: 0s - loss: 2.1695 - acc: 0.227 - ETA: 0s - loss: 2.1949 - acc: 0.21228/250 [==========================>...]231/250 [==========================>...] - ETA: 0s - loss: 2.1839 - acc: 0.2245 - ETA: 0s - loss: 2.1949 - acc: 0.222 - 3s 12ms/step - loss: 2.1592 - acc: 0.2304\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.1878 - acc: 0.2162239/250 [===========================>..] - ETA: 0s - loss: 2.1922 - acc: 0.2233236/250 [===========================>..] - ETA: 0s - loss: 2.1773 - acc: 0.226 - ETA: 0s - loss: 2.1807 - acc: 0.218 - ETA: 0s - loss: 2.1696 - acc: 0.228 - ETA: 0s - loss: 2.1861 - acc: 0.224 - 3s 12ms/step - loss: 2.1850 - acc: 0.2249\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 2.1655 - acc: 0.2292\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.1736 - acc: 0.220 - 3s 12ms/step - loss: 2.1742 - acc: 0.2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 2.1358 - acc: 0.280 - ETA: 1s - loss: 2.0203 - acc: 0.277 - ETA: 1s - loss: 1.9603 - acc: 0.3027 - ETA: 3s - loss: 2.0365 - acc: 0.300 - ETA: 3s - loss: 1.9510 - acc: 0.3600 23/250 [=>............................] - ETA: 1s - loss: 1.9570 - acc: 0.2887  1/250 [..............................] - ETA: 4s - loss: 2.0072 - acc: 0.140 - ETA: 1s - loss: 2.0126 - acc: 0.282  9/250 [>.............................] 31/250 [==>...........................] - ETA: 1s - loss: 1.9428 - acc: 0.295 - ETA: 1s - loss: 1.9721 - acc: 0.254 - ETA: 1s - loss: 1.9869 - acc: 0.280 - ETA: 1s - loss: 1.9378 - acc: 0.282 - ETA: 1s - loss: 1.9438 - acc: 0.29 - ETA: 1s - loss: 1.9703 - acc: 0.2547 - ETA: 1s - loss: 1.9777 - acc: 0.267 25/250 [==>...........................] - ETA: 1s - loss: 1.9517 - acc: 0.288 27/250 [==>...........................] - ETA: 1s - loss: 1.9738 - acc: 0.260 - ETA: 1s - loss: 1.9627 - acc: 0.27 - ETA: 1s - loss: 1.9539 - acc: 0.2812 - ETA: 1s - loss: 1.9643 - acc: 0.290 - ETA: 1s - loss: 1.9679 - acc: 0.262 - ETA: 1s - loss: 1.9739 - acc: 0.27 - ETA: 1s - loss: 1.9559 - acc: 0.2900 - ETA: 1s - loss: 1.9655 - acc: 0.287 - ETA: 1s - loss: 1.9588 - acc: 0.266 - ETA: 1s - loss: 1.9763 - acc: 0.27 - ETA: 1s - loss: 1.9578 - acc: 0.2864 - ETA: 1s - loss: 1.9562 - acc: 0.287 - ETA: 1s - loss: 1.9659 - acc: 0.265 - ETA: 1s - loss: 1.9696 - acc: 0.271 - ETA: 1s - loss: 1.9659 - acc: 0.282 - ETA: 1s - loss: 1.9572 - acc: 0.285 - ETA: 1s - loss: 1.9676 - acc: 0.264 - ETA: 1s - loss: 1.9805 - acc: 0.270 - ETA: 1s - loss: 1.9605 - acc: 0.2830 90/250 [=========>....................] - ETA: 1s - loss: 1.9482 - acc: 0.288 - ETA: 1s - loss: 1.9754 - acc: 0.264 - ETA: 1s - loss: 1.9779 - acc: 0.271 - ETA: 1s - loss: 1.9631 - acc: 0.285 - ETA: 1s - loss: 1.9476 - acc: 0.289 - ETA: 1s - loss: 1.9762 - acc: 0.266 - ETA: 1s - loss: 1.9757 - acc: 0.272 - ETA: 1s - loss: 1.9627 - acc: 0.281 87/250 [=========>....................] - ETA: 0s - loss: 1.9432 - acc: 0.2908 - ETA: 1s - loss: 1.9662 - acc: 0.267 - ETA: 1s - loss: 1.9722 - acc: 0.271 - ETA: 1s - loss: 1.9540 - acc: 0.28 - ETA: 0s - loss: 1.9379 - acc: 0.2917 95/250 [==========>...................] - ETA: 0s - loss: 1.9620 - acc: 0.2686 - ETA: 1s - loss: 1.9676 - acc: 0.27 - ETA: 0s - loss: 1.9562 - acc: 0.282124/250 [=============>................] - ETA: 0s - loss: 1.9575 - acc: 0.2704104/250 [===========>..................] - ETA: 0s - loss: 1.9354 - acc: 0.292 - ETA: 0s - loss: 1.9652 - acc: 0.270 - ETA: 0s - loss: 1.9595 - acc: 0.28133/250 [==============>...............] - ETA: 0s - loss: 1.9301 - acc: 0.2946112/250 [============>................. - ETA: 0s - loss: 1.9605 - acc: 0.2700 - ETA: 0s - loss: 1.9574 - acc: 0.275 - ETA: 0s - loss: 1.9518 - acc: 0.284141/250 [===============>..............]119/250 [=============>................] - ETA: 0s - loss: 1.9591 - acc: 0.272 - ETA: 0s - loss: 1.9536 - acc: 0.276 - ETA: 0s - loss: 1.9536 - acc: 0.283 - ETA: 0s - loss: 1.9215 - acc: 0.29 - ETA: 0s - loss: 1.9508 - acc: 0.2750 - ETA: 0s - loss: 1.9476 - acc: 0.277 - ETA: 0s - loss: 1.9480 - acc: 0.28136/250 [===============>..............]159/250 [==================>...........] - ETA: 0s - loss: 1.9236 - acc: 0.297 - ETA: 0s - loss: 1.9453 - acc: 0.2799 - ETA: 0s - loss: 1.9448 - acc: 0.278 - ETA: 0s - loss: 1.9468 - acc: 0.28167/250 [===================>..........145/250 [================>.............] - ETA: 0s - loss: 1.9421 - acc: 0.2790147/250 [================>.............] - ETA: 0s - loss: 1.9410 - acc: 0.281 - ETA: 0s - loss: 1.9525 - acc: 0.282 - ETA: 0s - loss: 1.9209 - acc: 0.296 - ETA: 0s - loss: 1.9357 - acc: 0.280 - ETA: 0s - loss: 1.9398 - acc: 0.280 - ETA: 0s - loss: 1.9517 - acc: 0.28162/250 [==================>...........] - ETA: 0s - loss: 1.9375 - acc: 0.281 - ETA: 0s - loss: 1.9402 - acc: 0.279 - ETA: 0s - loss: 1.9531 - acc: 0.281192/250 [======================>.......] - ETA: 0s - loss: 1.9168 - acc: 0.2973 - ETA: 0s - loss: 1.9330 - acc: 0.282 - ETA: 0s - loss: 1.9364 - acc: 0.280 - ETA: 0s - loss: 1.9490 - acc: 0.283 - ETA: 0s - loss: 1.9161 - acc: 0.297 - ETA: 0s - loss: 1.9334 - acc: 0.283 - ETA: 0s - loss: 1.9346 - acc: 0.283 - ETA: 0s - loss: 1.9479 - acc: 0.284 - ETA: 0s - loss: 1.9131 - acc: 0.298 - ETA: 0s - loss: 1.9325 - acc: 0.283 - ETA: 0s - loss: 1.9363 - acc: 0.283 - ETA: 0s - loss: 1.9468 - acc: 0.28216/250 [========================>.....] - ETA: 0s - loss: 1.9129 - acc: 0.2985194/250 [======================>.......] - ETA: 0s - loss: 1.9282 - acc: 0.284 - ETA: 0s - loss: 1.9302 - acc: 0.285 - ETA: 0s - loss: 1.9458 - acc: 0.28 - ETA: 0s - loss: 1.9114 - acc: 0.2994 - ETA: 0s - loss: 1.9236 - acc: 0.287 - ETA: 0s - loss: 1.9273 - acc: 0.285 - ETA: 0s - loss: 1.9430 - acc: 0.288 - ETA: 0s - loss: 1.9237 - acc: 0.287 - ETA: 0s - loss: 1.9222 - acc: 0.2888233/250 [==========================>...] - ETA: 0s - loss: 1.9066 - acc: 0.302 - ETA: 0s - loss: 1.9417 - acc: 0.288 - ETA: 0s - loss: 1.9225 - acc: 0.28242/250 [============================>.] - ETA: 0s - loss: 1.9041 - acc: 0.304 - ETA: 0s - loss: 1.9380 - acc: 0.28 - ETA: 0s - loss: 1.9172 - acc: 0.2895 - 2s 6ms/step - loss: 1.9054 - acc: 0.3040\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.9168 - acc: 0.289 - ETA: 0s - loss: 1.9148 - acc: 0.28 - ETA: 0s - loss: 1.9175 - acc: 0.2896 - ETA: 0s - loss: 1.9363 - acc: 0.291 - ETA: 0s - loss: 1.9360 - acc: 0.290244/250 [============================>. - ETA: 0s - loss: 1.9149 - acc: 0.29 - 2s 6ms/step - loss: 1.9123 - acc: 0.2918\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.9349 - acc: 0.291 - 2s 7ms/step - loss: 1.9146 - acc: 0.2912\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.9347 - acc: 0.2921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 2.0821 - acc: 0.280 - ETA: 1s - loss: 1.8664 - acc: 0.300 - ETA: 1s - loss: 1.8769 - acc: 0.316 - ETA: 1s - loss: 1.8805 - acc: 0.318 - ETA: 1s - loss: 1.8437 - acc: 0.3262 - ETA: 2s - loss: 1.8511 - acc: 0.3200 - ETA: 2s - loss: 1.8658 - acc: 0.320 37/250 [===>..........................] - ETA: 1s - loss: 1.8456 - acc: 0.3281  1/250 [..............................] - ETA: 1s - loss: 1.7254 - acc: 0.340 - ETA: 1s - loss: 1.8126 - acc: 0.346 - ETA: 1s - loss: 1.8220 - acc: 0.317  9/250 [>.............................] - ETA: 1s - loss: 1.9051 - acc: 0.3022 - ETA: 1s - loss: 1.8340 - acc: 0.337 - ETA: 1s - loss: 1.8546 - acc: 0.325 - ETA: 1s - loss: 1.8288 - acc: 0.33 - ETA: 1s - loss: 1.8639 - acc: 0.3082 - ETA: 1s - loss: 1.8330 - acc: 0.337 - ETA: 1s - loss: 1.8208 - acc: 0.327 - ETA: 1s - loss: 1.8206 - acc: 0.33 25/250 [==>...........................] 62/250 [======>.......................] - ETA: 1s - loss: 1.8379 - acc: 0.3365 - ETA: 1s - loss: 1.8563 - acc: 0.320 - ETA: 1s - loss: 1.8331 - acc: 0.316 - ETA: 1s - loss: 1.8101 - acc: 0.345 - ETA: 1s - loss: 1.8433 - acc: 0.320 - ETA: 1s - loss: 1.8290 - acc: 0.340 - ETA: 1s - loss: 1.8374 - acc: 0.318 - ETA: 1s - loss: 1.7959 - acc: 0.348 - ETA: 1s - loss: 1.8446 - acc: 0.317 - ETA: 1s - loss: 1.8204 - acc: 0.339 - ETA: 1s - loss: 1.8385 - acc: 0.315 - ETA: 1s - loss: 1.8055 - acc: 0.345 - ETA: 1s - loss: 1.8460 - acc: 0.318 - ETA: 1s - loss: 1.8261 - acc: 0.340 - ETA: 1s - loss: 1.8464 - acc: 0.317 - ETA: 1s - loss: 1.8036 - acc: 0.343 - ETA: 1s - loss: 1.8454 - acc: 0.316 - ETA: 1s - loss: 1.8239 - acc: 0.341 - ETA: 1s - loss: 1.8423 - acc: 0.322 - ETA: 1s - loss: 1.8070 - acc: 0.343 - ETA: 1s - loss: 1.8508 - acc: 0.315 - ETA: 1s - loss: 1.8250 - acc: 0.340 - ETA: 1s - loss: 1.8373 - acc: 0.324 - ETA: 1s - loss: 1.8048 - acc: 0.342 - ETA: 1s - loss: 1.8553 - acc: 0.314 - ETA: 0s - loss: 1.8298 - acc: 0.338 - ETA: 1s - loss: 1.8339 - acc: 0.326 - ETA: 1s - loss: 1.8007 - acc: 0.341 - ETA: 1s - loss: 1.8585 - acc: 0.312 - ETA: 0s - loss: 1.8275 - acc: 0.339 - ETA: 1s - loss: 1.8274 - acc: 0.329 - ETA: 1s - loss: 1.7942 - acc: 0.345 - ETA: 1s - loss: 1.8570 - acc: 0.313 - ETA: 0s - loss: 1.8271 - acc: 0.339 - ETA: 0s - loss: 1.8316 - acc: 0.329 - ETA: 0s - loss: 1.8051 - acc: 0.341 - ETA: 1s - loss: 1.8619 - acc: 0.310 - ETA: 0s - loss: 1.8250 - acc: 0.340 - ETA: 0s - loss: 1.8286 - acc: 0.331 - ETA: 0s - loss: 1.8000 - acc: 0.342 - ETA: 0s - loss: 1.8559 - acc: 0.312 - ETA: 0s - loss: 1.8250 - acc: 0.341 - ETA: 0s - loss: 1.8287 - acc: 0.329 - ETA: 0s - loss: 1.8007 - acc: 0.341 - ETA: 0s - loss: 1.8574 - acc: 0.31122/250 [=============>................] - ETA: 0s - loss: 1.8238 - acc: 0.332 - ETA: 0s - loss: 1.8006 - acc: 0.342 - ETA: 0s - loss: 1.8565 - acc: 0.312 - ETA: 0s - loss: 1.8188 - acc: 0.342 - ETA: 0s - loss: 1.8193 - acc: 0.332 - ETA: 0s - loss: 1.7959 - acc: 0.342 - ETA: 0s - loss: 1.8515 - acc: 0.315 - ETA: 0s - loss: 1.8196 - acc: 0.341 - ETA: 0s - loss: 1.8151 - acc: 0.334 - ETA: 0s - loss: 1.7973 - acc: 0.341 - ETA: 0s - loss: 1.8512 - acc: 0.317 - ETA: 0s - loss: 1.8145 - acc: 0.342 - ETA: 0s - loss: 1.8112 - acc: 0.335 - ETA: 0s - loss: 1.7946 - acc: 0.343 - ETA: 0s - loss: 1.8488 - acc: 0.316 - ETA: 0s - loss: 1.8116 - acc: 0.342 - ETA: 0s - loss: 1.8072 - acc: 0.335 - ETA: 0s - loss: 1.7942 - acc: 0.344 - ETA: 0s - loss: 1.8468 - acc: 0.317 - ETA: 0s - loss: 1.8077 - acc: 0.344 - ETA: 0s - loss: 1.8027 - acc: 0.337 - ETA: 0s - loss: 1.7924 - acc: 0.345 - ETA: 0s - loss: 1.8404 - acc: 0.319 - ETA: 0s - loss: 1.8016 - acc: 0.34171/250 [===================>..........] - ETA: 0s - loss: 1.7945 - acc: 0.344 - ETA: 0s - loss: 1.8406 - acc: 0.317 - ETA: 0s - loss: 1.7955 - acc: 0.34179/250 [====================>.........] - ETA: 0s - loss: 1.7983 - acc: 0.343 - ETA: 0s - loss: 1.8402 - acc: 0.318 - ETA: 0s - loss: 1.7937 - acc: 0.349187/250 [=====================>........] - ETA: 0s - loss: 1.7974 - acc: 0.3448 - ETA: 0s - loss: 1.7986 - acc: 0.339187/250 [=====================>........] - ETA: 0s - loss: 1.8359 - acc: 0.3225 - ETA: 0s - loss: 1.7912 - acc: 0.35195/250 [======================>.......]201/250 [=======================>......] - ETA: 0s - loss: 1.7967 - acc: 0.3465 - ETA: 0s - loss: 1.7972 - acc: 0.340 - ETA: 0s - loss: 1.8334 - acc: 0.325 - ETA: 0s - loss: 1.7882 - acc: 0.35210/250 [========================>.....] - ETA: 0s - loss: 1.7945 - acc: 0.341 - ETA: 0s - loss: 1.8314 - acc: 0.327 - ETA: 0s - loss: 1.7840 - acc: 0.35 - ETA: 0s - loss: 1.7912 - acc: 0.3426 - ETA: 0s - loss: 1.7927 - acc: 0.346213/250 [========================>.....]249/250 [============================>.] - ETA: 0s - loss: 1.7807 - acc: 0.352 - 2s 7ms/step - loss: 1.7809 - acc: 0.3529\n",
      "221/250 [=========================>....]] - ETA: 0s - loss: 1.7877 - acc: 0.3447 - ETA: 0s - loss: 1.7883 - acc: 0.347\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 1.8248 - acc: 0.329 - ETA: 0s - loss: 1.7868 - acc: 0.3462229/250 [==========================>...] - ETA: 0s - loss: 1.7891 - acc: 0.346 - ETA: 0s - loss: 1.8220 - acc: 0.32238/250 [===========================>..] - ETA: 0s - loss: 1.7858 - acc: 0.347 - ETA: 0s - loss: 1.8188 - acc: 0.332 - 2s 6ms/step - loss: 1.7802 - acc: 0.3471\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.7837 - acc: 0.348 - ETA: 0s - loss: 1.8156 - acc: 0.333 - 2s 6ms/step - loss: 1.7822 - acc: 0.3493\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.8163 - acc: 0.3341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Received) round: b'1:0', device: 0\n",
      "(Received) round: b'1:1', device: 1\n",
      "(Received) round: b'1:2', device: 2\n",
      "(Received) round: b'1:3', device: 3\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 5:48 - loss: 21.2555 - acc: 0.2200 - ETA: 5:51 - loss: 18.0927 - acc: 0.3400 - ETA: 5:51 - loss: 21.9592 - acc: 0.2600 - ETA: 5:51 - loss: 20.7013 - acc: 0.28 - ETA: 26s - loss: 5.2864 - acc: 0.1138 - ETA: 35s - loss: 6.5802 - acc: 0.182 - ETA: 31s - loss: 5.3875 - acc: 0.1255   12/250 [>.............................] - ETA: 29s - loss: 4.5641 - acc: 0.1183 - ETA: 16s - loss: 4.1682 - acc: 0.11 - ETA: 18s - loss: 4.6399 - acc: 0.14 - ETA: 16s - loss: 3.9375 - acc: 0.12 - ETA: 15s - loss: 3.5630 - acc: 0.11 - ETA: 10s - loss: 3.4936 - acc: 0.12 - ETA: 13s - loss: 4.0417 - acc: 0.13 - ETA: 10s - loss: 3.3982 - acc: 0.12 - ETA: 11s - loss: 3.2167 - acc: 0.11 - ETA: 8s - loss: 3.2606 - acc: 0.125 - ETA: 9s - loss: 3.5788 - acc: 0.132 - ETA: 8s - loss: 3.1651 - acc: 0.120 - ETA: 8s - loss: 3.0463 - acc: 0.1097 - ETA: 6s - loss: 3.1109 - acc: 0.122 - ETA: 7s - loss: 3.2936 - acc: 0.129 - ETA: 6s - loss: 3.0266 - acc: 0.116 - ETA: 7s - loss: 2.9220 - acc: 0.108 - ETA: 5s - loss: 2.9934 - acc: 0.120 - ETA: 5s - loss: 3.1370 - acc: 0.129 - ETA: 5s - loss: 2.9176 - acc: 0.116 - ETA: 6s - loss: 2.8344 - acc: 0.105 - ETA: 4s - loss: 2.9113 - acc: 0.117 - ETA: 5s - loss: 3.0369 - acc: 0.128 - ETA: 4s - loss: 2.8420 - acc: 0.120 - ETA: 5s - loss: 2.7638 - acc: 0.105 - ETA: 4s - loss: 2.8405 - acc: 0.115 - ETA: 4s - loss: 2.9491 - acc: 0.126 - ETA: 4s - loss: 2.7863 - acc: 0.119 - ETA: 4s - loss: 2.7176 - acc: 0.104 - ETA: 3s - loss: 2.7812 - acc: 0.116 - ETA: 3s - loss: 2.8887 - acc: 0.125 - ETA: 3s - loss: 2.7352 - acc: 0.120 - ETA: 4s - loss: 2.6824 - acc: 0.105 - ETA: 3s - loss: 2.7418 - acc: 0.116 - ETA: 3s - loss: 2.8382 - acc: 0.123 - ETA: 3s - loss: 2.7029 - acc: 0.119 - ETA: 3s - loss: 2.6519 - acc: 0.106 - ETA: 2s - loss: 2.7058 - acc: 0.116 - ETA: 3s - loss: 2.7904 - acc: 0.121 99/250 [==========>...................] 95/250 [==========>...................] - ETA: 3s - loss: 2.6271 - acc: 0.106 - ETA: 2s - loss: 2.6697 - acc: 0.118 - ETA: 2s - loss: 2.7541 - acc: 0.120108/250 [===========>..................] - ETA: 2s - loss: 2.6573 - acc: 0.1181 - ETA: 2s - loss: 2.6040 - acc: 0.106 - ETA: 2s - loss: 2.6440 - acc: 0.118 - ETA: 2s - loss: 2.7275 - acc: 0.119 - ETA: 2s - loss: 2.6311 - acc: 0.117 - ETA: 2s - loss: 2.5798 - acc: 0.108 - ETA: 2s - loss: 2.6224 - acc: 0.119 - ETA: 2s - loss: 2.6971 - acc: 0.118 - ETA: 2s - loss: 2.6091 - acc: 0.116 - ETA: 2s - loss: 2.5610 - acc: 0.108 - ETA: 1s - loss: 2.5997 - acc: 0.119 - ETA: 1s - loss: 2.6744 - acc: 0.116 - ETA: 1s - loss: 2.5915 - acc: 0.115 - ETA: 2s - loss: 2.5458 - acc: 0.108 - ETA: 1s - loss: 2.5804 - acc: 0.120 - ETA: 1s - loss: 2.6531 - acc: 0.116 - ETA: 1s - loss: 2.5757 - acc: 0.115 - ETA: 1s - loss: 2.5280 - acc: 0.108151/250 [=================>............] - ETA: 1s - loss: 2.6339 - acc: 0.1161 - ETA: 1s - loss: 2.5618 - acc: 0.120 - ETA: 1s - loss: 2.5616 - acc: 0.114 - ETA: 1s - loss: 2.5122 - acc: 0.109160/250 [==================>...........]155/250 [=================>............] - ETA: 1s - loss: 2.6198 - acc: 0.116 - ETA: 1s - loss: 2.5472 - acc: 0.114 - ETA: 1s - loss: 2.4971 - acc: 0.110163/250 [==================>...........] - ETA: 1s - loss: 2.5319 - acc: 0.1217 - ETA: 1s - loss: 2.6034 - acc: 0.115 - ETA: 1s - loss: 2.5356 - acc: 0.113 - ETA: 1s - loss: 2.4868 - acc: 0.111177/250 [====================>.........] - ETA: 1s - loss: 2.5172 - acc: 0.1227 - ETA: 1s - loss: 2.5877 - acc: 0.115 - ETA: 1s - loss: 2.5234 - acc: 0.114 - ETA: 1s - loss: 2.4754 - acc: 0.113 - ETA: 0s - loss: 2.5058 - acc: 0.123 - ETA: 0s - loss: 2.5744 - acc: 0.115 - ETA: 0s - loss: 2.5142 - acc: 0.114 - ETA: 1s - loss: 2.4655 - acc: 0.115 - ETA: 0s - loss: 2.4949 - acc: 0.124 - ETA: 0s - loss: 2.5617 - acc: 0.115 - ETA: 0s - loss: 2.5058 - acc: 0.114 - ETA: 0s - loss: 2.4577 - acc: 0.114 - ETA: 0s - loss: 2.4858 - acc: 0.124 - ETA: 0s - loss: 2.5516 - acc: 0.114 - ETA: 0s - loss: 2.4977 - acc: 0.114 - ETA: 0s - loss: 2.4519 - acc: 0.114 - ETA: 0s - loss: 2.4763 - acc: 0.125 - ETA: 0s - loss: 2.5421 - acc: 0.114 - ETA: 0s - loss: 2.4894 - acc: 0.113 - ETA: 0s - loss: 2.4455 - acc: 0.113 - ETA: 0s - loss: 2.4656 - acc: 0.126 - ETA: 0s - loss: 2.5338 - acc: 0.113 - ETA: 0s - loss: 2.4797 - acc: 0.113 - ETA: 0s - loss: 2.4392 - acc: 0.114 - ETA: 0s - loss: 2.4573 - acc: 0.127 - ETA: 0s - loss: 2.5256 - acc: 0.113220/250 [=========================>....] - ETA: 0s - loss: 2.4328 - acc: 0.1156 - ETA: 0s - loss: 2.4727 - acc: 0.114 - ETA: 0s - loss: 2.4486 - acc: 0.128 - ETA: 0s - loss: 2.5173 - acc: 0.112 - ETA: 0s - loss: 2.4646 - acc: 0.115 - ETA: 0s - loss: 2.4269 - acc: 0.116 - ETA: 0s - loss: 2.4412 - acc: 0.131 - ETA: 0s - loss: 2.5091 - acc: 0.11 - ETA: 0s - loss: 2.4211 - acc: 0.1165 - ETA: 0s - loss: 2.4587 - acc: 0.116 - ETA: 0s - loss: 2.4337 - acc: 0.13 - 3s 12ms/step - loss: 2.4331 - acc: 0.1330\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.5022 - acc: 0.113 - 3s 12ms/step - loss: 2.4536 - acc: 0.1154\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 2.5008 - acc: 0.1134\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.4140 - acc: 0.118 - 3s 12ms/step - loss: 2.4098 - acc: 0.1189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 2.1836 - acc: 0.2000 - ETA: 2s - loss: 2.2618 - acc: 0.1800 - ETA: 2s - loss: 2.2860 - acc: 0.080  1/250 [..............................] - ETA: 2s - loss: 2.2900 - acc: 0.12  9/250 [>.............................]  9/250 [>.............................] - ETA: 1s - loss: 2.3036 - acc: 0.1067 - ETA: 1s - loss: 2.2709 - acc: 0.14 - ETA: 1s - loss: 2.2729 - acc: 0.1365 - ETA: 1s - loss: 2.2659 - acc: 0.12 - ETA: 1s - loss: 2.2838 - acc: 0.1600 - ETA: 1s - loss: 2.2944 - acc: 0.114 - ETA: 1s - loss: 2.2455 - acc: 0.141 - ETA: 1s - loss: 2.2533 - acc: 0.130 - ETA: 1s - loss: 2.2900 - acc: 0.1160 25/250 [==>...........................] - ETA: 1s - loss: 2.2847 - acc: 0.14 - ETA: 1s - loss: 2.2362 - acc: 0.1467 - ETA: 1s - loss: 2.2466 - acc: 0.14 33/250 [==>...........................] - ETA: 1s - loss: 2.2770 - acc: 0.1261 32/250 [==>...........................] - ETA: 1s - loss: 2.2806 - acc: 0.145 - ETA: 1s - loss: 2.2276 - acc: 0.146 - ETA: 1s - loss: 2.2448 - acc: 0.14 42/250 [====>.........................] - ETA: 1s - loss: 2.2692 - acc: 0.1295 43/250 [====>.........................] - ETA: 1s - loss: 2.2947 - acc: 0.144 - ETA: 1s - loss: 2.2164 - acc: 0.149 - ETA: 1s - loss: 2.2398 - acc: 0.151 51/250 [=====>........................] - ETA: 1s - loss: 2.2905 - acc: 0.1404 - ETA: 1s - loss: 2.2718 - acc: 0.125 - ETA: 1s - loss: 2.2047 - acc: 0.153 - ETA: 1s - loss: 2.2371 - acc: 0.150 - ETA: 1s - loss: 2.2849 - acc: 0.142 - ETA: 1s - loss: 2.2728 - acc: 0.124 66/250 [======>.......................] - ETA: 1s - loss: 2.1983 - acc: 0.1506 - ETA: 1s - loss: 2.2287 - acc: 0.153 - ETA: 1s - loss: 2.2852 - acc: 0.144 - ETA: 1s - loss: 2.2726 - acc: 0.123 - ETA: 1s - loss: 2.1927 - acc: 0.152 - ETA: 1s - loss: 2.2214 - acc: 0.15 75/250 [========>.....................] 75/250 [========>.....................] - ETA: 1s - loss: 2.2870 - acc: 0.1443 - ETA: 1s - loss: 2.2694 - acc: 0.126 - ETA: 1s - loss: 2.1912 - acc: 0.152 - ETA: 1s - loss: 2.2165 - acc: 0.15 - ETA: 1s - loss: 2.2868 - acc: 0.1443 - ETA: 1s - loss: 2.2763 - acc: 0.126 - ETA: 1s - loss: 2.1973 - acc: 0.151 - ETA: 1s - loss: 2.2118 - acc: 0.155 - ETA: 1s - loss: 2.2848 - acc: 0.142 - ETA: 1s - loss: 2.2767 - acc: 0.125 - ETA: 1s - loss: 2.1976 - acc: 0.152 - ETA: 1s - loss: 2.2119 - acc: 0.153 - ETA: 0s - loss: 2.2842 - acc: 0.141 - ETA: 0s - loss: 2.2768 - acc: 0.125 - ETA: 0s - loss: 2.1915 - acc: 0.152 - ETA: 0s - loss: 2.2134 - acc: 0.154 - ETA: 0s - loss: 2.2823 - acc: 0.140 - ETA: 0s - loss: 2.2762 - acc: 0.125 - ETA: 0s - loss: 2.1902 - acc: 0.154 - ETA: 0s - loss: 2.2161 - acc: 0.152 - ETA: 0s - loss: 2.2814 - acc: 0.141 - ETA: 0s - loss: 2.2769 - acc: 0.123 - ETA: 0s - loss: 2.1897 - acc: 0.157 - ETA: 0s - loss: 2.2139 - acc: 0.152 - ETA: 0s - loss: 2.2788 - acc: 0.144 - ETA: 0s - loss: 2.2749 - acc: 0.125 - ETA: 0s - loss: 2.1897 - acc: 0.157 - ETA: 0s - loss: 2.2118 - acc: 0.155 - ETA: 0s - loss: 2.2744 - acc: 0.14140/250 [===============>..............] - ETA: 0s - loss: 2.1906 - acc: 0.157 - ETA: 0s - loss: 2.2121 - acc: 0.155 - ETA: 0s - loss: 2.2705 - acc: 0.148 - ETA: 0s - loss: 2.2712 - acc: 0.1256148/250 [================>.............] - ETA: 0s - loss: 2.1923 - acc: 0.158 - ETA: 0s - loss: 2.2118 - acc: 0.155 - ETA: 0s - loss: 2.2664 - acc: 0.149 - ETA: 0s - loss: 2.2705 - acc: 0.126 - ETA: 0s - loss: 2.1880 - acc: 0.159 - ETA: 0s - loss: 2.2111 - acc: 0.154 - ETA: 0s - loss: 2.2667 - acc: 0.149 - ETA: 0s - loss: 2.2716 - acc: 0.125 - ETA: 0s - loss: 2.1878 - acc: 0.159 - ETA: 0s - loss: 2.2098 - acc: 0.155 - ETA: 0s - loss: 2.2644 - acc: 0.149 - ETA: 0s - loss: 2.2712 - acc: 0.127 - ETA: 0s - loss: 2.1870 - acc: 0.159 - ETA: 0s - loss: 2.2059 - acc: 0.157 - ETA: 0s - loss: 2.2621 - acc: 0.149 - ETA: 0s - loss: 2.2705 - acc: 0.12173/250 [===================>..........] - ETA: 0s - loss: 2.2067 - acc: 0.157 - ETA: 0s - loss: 2.2587 - acc: 0.149 - ETA: 0s - loss: 2.2704 - acc: 0.12181/250 [====================>.........]190/250 [=====================>........] - ETA: 0s - loss: 2.2071 - acc: 0.1573 - ETA: 0s - loss: 2.1834 - acc: 0.161 - ETA: 0s - loss: 2.2566 - acc: 0.149 - ETA: 0s - loss: 2.2706 - acc: 0.128198/250 [======================>.......] - ETA: 0s - loss: 2.2071 - acc: 0.1571 - ETA: 0s - loss: 2.1826 - acc: 0.162 - ETA: 0s - loss: 2.2539 - acc: 0.151 - ETA: 0s - loss: 2.2711 - acc: 0.129198/250 [======================>.......] - ETA: 0s - loss: 2.1812 - acc: 0.1617 - ETA: 0s - loss: 2.2048 - acc: 0.157 - ETA: 0s - loss: 2.2502 - acc: 0.153 - ETA: 0s - loss: 2.2721 - acc: 0.129 - ETA: 0s - loss: 2.1815 - acc: 0.161 - ETA: 0s - loss: 2.2029 - acc: 0.158 - ETA: 0s - loss: 2.2483 - acc: 0.153 - ETA: 0s - loss: 2.2722 - acc: 0.127 - ETA: 0s - loss: 2.1838 - acc: 0.162 - ETA: 0s - loss: 2.2044 - acc: 0.156 - ETA: 0s - loss: 2.2437 - acc: 0.155 - ETA: 0s - loss: 2.2727 - acc: 0.127 - ETA: 0s - loss: 2.1855 - acc: 0.162 - ETA: 0s - loss: 2.2029 - acc: 0.159 - ETA: 0s - loss: 2.2427 - acc: 0.156 - ETA: 0s - loss: 2.2731 - acc: 0.126 - ETA: 0s - loss: 2.1861 - acc: 0.160 - ETA: 0s - loss: 2.2009 - acc: 0.159 - ETA: 0s - loss: 2.2381 - acc: 0.157 - ETA: 0s - loss: 2.2733 - acc: 0.12241/250 [===========================>..] - ETA: 0s - loss: 2.2000 - acc: 0.16 - 2s 6ms/step - loss: 2.1876 - acc: 0.1602\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.2364 - acc: 0.158 - 2s 6ms/step - loss: 2.2356 - acc: 0.1584\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.2733 - acc: 0.128 - 2s 6ms/step - loss: 2.2732 - acc: 0.1280\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 2.1978 - acc: 0.1611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 2.1471 - acc: 0.2000 - ETA: 2s - loss: 1.9350 - acc: 0.280 - ETA: 1s - loss: 2.1901 - acc: 0.1650  1/250 [..............................]  1/250 [..............................] - ETA: 3s - loss: 2.2610 - acc: 0.1200 - ETA: 2s - loss: 2.2196 - acc: 0.140 - ETA: 1s - loss: 2.0536 - acc: 0.232 - ETA: 1s - loss: 2.1876 - acc: 0.17  9/250 [>.............................] - ETA: 1s - loss: 2.2875 - acc: 0.128 - ETA: 1s - loss: 2.0982 - acc: 0.217 - ETA: 1s - loss: 2.1785 - acc: 0.163 17/250 [=>............................] - ETA: 1s - loss: 2.1633 - acc: 0.1647 - ETA: 1s - loss: 2.2744 - acc: 0.140 - ETA: 1s - loss: 2.1056 - acc: 0.215 - ETA: 1s - loss: 2.1806 - acc: 0.158 - ETA: 1s - loss: 2.1773 - acc: 0.160 - ETA: 1s - loss: 2.2725 - acc: 0.13 - ETA: 1s - loss: 2.1299 - acc: 0.2094 - ETA: 1s - loss: 2.1696 - acc: 0.161 - ETA: 1s - loss: 2.1703 - acc: 0.168 - ETA: 1s - loss: 2.2764 - acc: 0.13 - ETA: 1s - loss: 2.1191 - acc: 0.2033 - ETA: 1s - loss: 2.1637 - acc: 0.167 - ETA: 1s - loss: 2.1749 - acc: 0.169 - ETA: 1s - loss: 2.2756 - acc: 0.132 - ETA: 1s - loss: 2.1119 - acc: 0.202 - ETA: 1s - loss: 2.1544 - acc: 0.173 - ETA: 1s - loss: 2.1723 - acc: 0.174 - ETA: 1s - loss: 2.2734 - acc: 0.132 59/250 [======>.......................] - ETA: 1s - loss: 2.1214 - acc: 0.1973 - ETA: 1s - loss: 2.1567 - acc: 0.173 - ETA: 1s - loss: 2.1711 - acc: 0.170 - ETA: 1s - loss: 2.2729 - acc: 0.128 68/250 [=======>......................] - ETA: 1s - loss: 2.1191 - acc: 0.1956 - ETA: 1s - loss: 2.1531 - acc: 0.175 - ETA: 1s - loss: 2.1725 - acc: 0.166 - ETA: 1s - loss: 2.2709 - acc: 0.131 80/250 [========>..................... - ETA: 1s - loss: 2.1210 - acc: 0.1951 75/250 [========>.....................] - ETA: 1s - loss: 2.1719 - acc: 0.166 - ETA: 1s - loss: 2.2720 - acc: 0.13 - ETA: 1s - loss: 2.1492 - acc: 0.1775 - ETA: 1s - loss: 2.1198 - acc: 0.196 - ETA: 1s - loss: 2.1682 - acc: 0.168 - ETA: 1s - loss: 2.2719 - acc: 0.132 - ETA: 1s - loss: 2.1494 - acc: 0.174 - ETA: 0s - loss: 2.1148 - acc: 0.194 - ETA: 1s - loss: 2.1619 - acc: 0.168 - ETA: 1s - loss: 2.2719 - acc: 0.129 - ETA: 0s - loss: 2.1475 - acc: 0.176 - ETA: 0s - loss: 2.1112 - acc: 0.195 - ETA: 0s - loss: 2.1573 - acc: 0.169 - ETA: 0s - loss: 2.2727 - acc: 0.12110/250 [============>.................] - ETA: 0s - loss: 2.1093 - acc: 0.195 - ETA: 0s - loss: 2.1550 - acc: 0.167 - ETA: 0s - loss: 2.2724 - acc: 0.12 - ETA: 0s - loss: 2.1037 - acc: 0.1980 - ETA: 0s - loss: 2.1455 - acc: 0.179 - ETA: 0s - loss: 2.1529 - acc: 0.167 - ETA: 0s - loss: 2.2714 - acc: 0.126 - ETA: 0s - loss: 2.1022 - acc: 0.201 - ETA: 0s - loss: 2.1427 - acc: 0.179 - ETA: 0s - loss: 2.1512 - acc: 0.166 - ETA: 0s - loss: 2.2705 - acc: 0.12137/250 [===============>..............]134/250 [===============>..............] - ETA: 0s - loss: 2.1434 - acc: 0.1810 - ETA: 0s - loss: 2.0984 - acc: 0.201 - ETA: 0s - loss: 2.1508 - acc: 0.166 - ETA: 0s - loss: 2.2696 - acc: 0.125 - ETA: 0s - loss: 2.0977 - acc: 0.202 - ETA: 0s - loss: 2.1430 - acc: 0.179 - ETA: 0s - loss: 2.1515 - acc: 0.168 - ETA: 0s - loss: 2.2686 - acc: 0.12155/250 [=================>............ - ETA: 0s - loss: 2.1488 - acc: 0.1685 - ETA: 0s - loss: 2.1432 - acc: 0.179 - ETA: 0s - loss: 2.2676 - acc: 0.12 - ETA: 0s - loss: 2.0978 - acc: 0.205 - ETA: 0s - loss: 2.1431 - acc: 0.1783 - ETA: 0s - loss: 2.1466 - acc: 0.168 - ETA: 0s - loss: 2.2678 - acc: 0.128 - ETA: 0s - loss: 2.0974 - acc: 0.20 - ETA: 0s - loss: 2.1404 - acc: 0.1800 - ETA: 0s - loss: 2.1455 - acc: 0.166 - ETA: 0s - loss: 2.2672 - acc: 0.129 - ETA: 0s - loss: 2.0975 - acc: 0.20 - ETA: 0s - loss: 2.1435 - acc: 0.1673 - ETA: 0s - loss: 2.1372 - acc: 0.181 - ETA: 0s - loss: 2.2663 - acc: 0.129 - ETA: 0s - loss: 2.0907 - acc: 0.207 - ETA: 0s - loss: 2.1438 - acc: 0.166 - ETA: 0s - loss: 2.1351 - acc: 0.181 - ETA: 0s - loss: 2.2658 - acc: 0.128 - ETA: 0s - loss: 2.0900 - acc: 0.20198/250 [======================>.......] - ETA: 0s - loss: 2.1344 - acc: 0.182 - ETA: 0s - loss: 2.2654 - acc: 0.128 - ETA: 0s - loss: 2.0844 - acc: 0.209 - ETA: 0s - loss: 2.1342 - acc: 0.181 - ETA: 0s - loss: 2.1387 - acc: 0.167 - ETA: 0s - loss: 2.2668 - acc: 0.128 - ETA: 0s - loss: 2.0841 - acc: 0.209 - ETA: 0s - loss: 2.1326 - acc: 0.181 - ETA: 0s - loss: 2.1372 - acc: 0.167 - ETA: 0s - loss: 2.2658 - acc: 0.128 - ETA: 0s - loss: 2.0816 - acc: 0.209 - ETA: 0s - loss: 2.1314 - acc: 0.181 - ETA: 0s - loss: 2.1379 - acc: 0.167 - ETA: 0s - loss: 2.2637 - acc: 0.129 - ETA: 0s - loss: 2.0768 - acc: 0.211 - ETA: 0s - loss: 2.1302 - acc: 0.181 - ETA: 0s - loss: 2.1370 - acc: 0.167 - ETA: 0s - loss: 2.2632 - acc: 0.130 - ETA: 0s - loss: 2.0765 - acc: 0.212 - ETA: 0s - loss: 2.1301 - acc: 0.181 - ETA: 0s - loss: 2.1367 - acc: 0.167 - ETA: 0s - loss: 2.2625 - acc: 0.131 - ETA: 0s - loss: 2.0747 - acc: 0.213 - ETA: 0s - loss: 2.1287 - acc: 0.182 - ETA: 0s - loss: 2.1357 - acc: 0.167 - 2s 6ms/step - loss: 2.1286 - acc: 0.1820\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.2611 - acc: 0.132 - ETA: 0s - loss: 2.0740 - acc: 0.214 - ETA: 0s - loss: 2.1347 - acc: 0.16 - 2s 6ms/step - loss: 2.1344 - acc: 0.1690\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 2.2591 - acc: 0.1328\n",
      "250/250 [==============================]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 2s 6ms/step - loss: 2.0731 - acc: 0.2139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Received) round: b'2:0', device: 0\n",
      "(Received) round: b'2:1', device: 1\n",
      "(Received) round: b'2:2', device: 2\n",
      "(Received) round: b'2:3', device: 3\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s 12ms/step - loss: 2.0693 - acc: 0.2163TA: 5:40 - loss: 2.3019 - acc: 0.100 12/250 [>.............................]  1/250 [..............................] - ETA: 28s - loss: 2.2159 - acc: 0.1700  - ETA: 5:48 - loss: 2.2980 - acc: 0.0800 - ETA: 5:51 - loss: 2.3167 - acc: 0.0600 - ETA: 5:51 - loss: 2.3236 - acc: 0.0 - ETA: 16s - loss: 2.2121 - acc: 0.1495 - ETA: 28s - loss: 2.2550 - acc: 0.1300 - ETA: 29s - loss: 2.2451 - acc: 0.1067 - ETA: 35s - loss: 2.2344 - acc: 0.1 - ETA: 16s - loss: 2.2269 - acc: 0.1467 - ETA: 10s - loss: 2.1856 - acc: 0.15 - ETA: 14s - loss: 2.2528 - acc: 0.13 - ETA: 17s - loss: 2.2195 - acc: 0. - ETA: 7s - loss: 2.1750 - acc: 0.1733  - ETA: 11s - loss: 2.2054 - acc: 0.15 - ETA: 10s - loss: 2.2331 - acc: 0.13 - ETA: 11s - loss: 2.2079 - acc: 0.145 37/250 [===>..........................] - ETA: 9s - loss: 2.1877 - acc: 0.1584  50/250 [=====>........................] - ETA: 6s - loss: 2.1690 - acc: 0.17 - ETA: 8s - loss: 2.2118 - acc: 0.152 - ETA: 8s - loss: 2.1903 - acc: 0.157 58/250 [=====>........................] - ETA: 5s - loss: 2.1584 - acc: 0.176 - ETA: 7s - loss: 2.1966 - acc: 0.159 - ETA: 6s - loss: 2.1771 - acc: 0.16 58/250 [=====>........................] - ETA: 5s - loss: 2.1767 - acc: 0.164 - ETA: 5s - loss: 2.1931 - acc: 0.163 - ETA: 5s - loss: 2.1669 - acc: 0.17 - ETA: 4s - loss: 2.1422 - acc: 0.1816 - ETA: 4s - loss: 2.1668 - acc: 0.170 - ETA: 5s - loss: 2.1874 - acc: 0.166 - ETA: 5s - loss: 2.1628 - acc: 0.172 75/250 [========>.....................] - ETA: 4s - loss: 2.1634 - acc: 0.1736 - ETA: 3s - loss: 2.1407 - acc: 0.185 - ETA: 4s - loss: 2.1809 - acc: 0.168 - ETA: 4s - loss: 2.1585 - acc: 0.17 83/250 [========>.....................] - ETA: 3s - loss: 2.1569 - acc: 0.1788 93/250 [==========>...................] - ETA: 3s - loss: 2.1317 - acc: 0.189 - ETA: 3s - loss: 2.1770 - acc: 0.165 - ETA: 3s - loss: 2.1522 - acc: 0.181 - ETA: 3s - loss: 2.1540 - acc: 0.18 - ETA: 2s - loss: 2.1268 - acc: 0.1900 - ETA: 3s - loss: 2.1685 - acc: 0.169 - ETA: 3s - loss: 2.1406 - acc: 0.187 - ETA: 3s - loss: 2.1496 - acc: 0.182 - ETA: 2s - loss: 2.1234 - acc: 0.189 - ETA: 3s - loss: 2.1645 - acc: 0.171 - ETA: 3s - loss: 2.1367 - acc: 0.188 - ETA: 2s - loss: 2.1467 - acc: 0.181 - ETA: 2s - loss: 2.1212 - acc: 0.191 - ETA: 2s - loss: 2.1548 - acc: 0.176 - ETA: 2s - loss: 2.1300 - acc: 0.193 - ETA: 2s - loss: 2.1439 - acc: 0.183 - ETA: 2s - loss: 2.1193 - acc: 0.190 - ETA: 2s - loss: 2.1521 - acc: 0.176 - ETA: 2s - loss: 2.1235 - acc: 0.19 - ETA: 2s - loss: 2.1401 - acc: 0.1839 - ETA: 1s - loss: 2.1204 - acc: 0.191 - ETA: 2s - loss: 2.1502 - acc: 0.177 - ETA: 2s - loss: 2.1206 - acc: 0.19140/250 [===============>..............] - ETA: 1s - loss: 2.1199 - acc: 0.1907132/250 [==============>............... - ETA: 1s - loss: 2.1397 - acc: 0.1856 - ETA: 2s - loss: 2.1545 - acc: 0.180 - ETA: 1s - loss: 2.1166 - acc: 0.197148/250 [================>.............] - ETA: 1s - loss: 2.1161 - acc: 0.1932 - ETA: 1s - loss: 2.1370 - acc: 0.186 - ETA: 1s - loss: 2.1502 - acc: 0.182 - ETA: 1s - loss: 2.1080 - acc: 0.20148/250 [================>.............] - ETA: 1s - loss: 2.1310 - acc: 0.190 - ETA: 1s - loss: 2.1481 - acc: 0.183 - ETA: 1s - loss: 2.1023 - acc: 0.203 - ETA: 1s - loss: 2.1046 - acc: 0.197 - ETA: 1s - loss: 2.1287 - acc: 0.191 - ETA: 1s - loss: 2.1442 - acc: 0.182 - ETA: 1s - loss: 2.0986 - acc: 0.204 - ETA: 1s - loss: 2.1020 - acc: 0.199 - ETA: 1s - loss: 2.1264 - acc: 0.193 - ETA: 1s - loss: 2.1451 - acc: 0.182 - ETA: 1s - loss: 2.0971 - acc: 0.206 - ETA: 0s - loss: 2.1023 - acc: 0.19174/250 [===================>..........] - ETA: 1s - loss: 2.1427 - acc: 0.183 - ETA: 1s - loss: 2.0892 - acc: 0.209 - ETA: 0s - loss: 2.0993 - acc: 0.198 - ETA: 0s - loss: 2.1167 - acc: 0.196 - ETA: 0s - loss: 2.1396 - acc: 0.186 - ETA: 0s - loss: 2.0847 - acc: 0.212 - ETA: 0s - loss: 2.0911 - acc: 0.20 - ETA: 0s - loss: 2.1147 - acc: 0.1972 - ETA: 0s - loss: 2.1353 - acc: 0.188 - ETA: 0s - loss: 2.0809 - acc: 0.212 - ETA: 0s - loss: 2.0878 - acc: 0.202 - ETA: 0s - loss: 2.1131 - acc: 0.197 - ETA: 0s - loss: 2.1340 - acc: 0.18 - ETA: 0s - loss: 2.0752 - acc: 0.2152 - ETA: 0s - loss: 2.0844 - acc: 0.205 - ETA: 0s - loss: 2.1131 - acc: 0.198 - ETA: 0s - loss: 2.1341 - acc: 0.189210/250 [========================>.....]220/250 [=========================>....] - ETA: 0s - loss: 2.0809 - acc: 0.207 - ETA: 0s - loss: 2.1100 - acc: 0.200 - ETA: 0s - loss: 2.1303 - acc: 0.191 - ETA: 0s - loss: 2.0654 - acc: 0.220 - ETA: 0s - loss: 2.0799 - acc: 0.209 - ETA: 0s - loss: 2.1035 - acc: 0.204 - ETA: 0s - loss: 2.1286 - acc: 0.192 - ETA: 0s - loss: 2.0580 - acc: 0.223 - ETA: 0s - loss: 2.0797 - acc: 0.209 - ETA: 0s - loss: 2.0986 - acc: 0.206 - ETA: 0s - loss: 2.1272 - acc: 0.192 - ETA: 0s - loss: 2.0530 - acc: 0.226 - ETA: 0s - loss: 2.0737 - acc: 0.213 - ETA: 0s - loss: 2.0917 - acc: 0.210 - ETA: 0s - loss: 2.1231 - acc: 0.194250/250 [==============================]\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.0446 - acc: 0.231 - ETA: 0s - loss: 2.1177 - acc: 0.196 - 3s 12ms/step - loss: 2.1175 - acc: 0.1967\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 2.0414 - acc: 0.2340\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 2.0892 - acc: 0.2122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 1.8128 - acc: 0.2800  8/250 [..............................] - ETA: 2s - loss: 2.0105 - acc: 0.2400 - ETA: 1s - loss: 1.8975 - acc: 0.3175 - ETA: 2s - loss: 1.8555 - acc: 0.2800 - ETA: 2s - loss: 1.8216 - acc: 0.36 16/250 [>.............................] - ETA: 1s - loss: 1.9065 - acc: 0.3075  9/250 [>.............................] - ETA: 1s - loss: 1.9315 - acc: 0.29 - ETA: 1s - loss: 1.9256 - acc: 0.2778 - ETA: 1s - loss: 1.9666 - acc: 0.284 - ETA: 1s - loss: 1.9561 - acc: 0.282 - ETA: 1s - loss: 1.9177 - acc: 0.30 - ETA: 1s - loss: 1.9073 - acc: 0.3082 - ETA: 1s - loss: 1.9551 - acc: 0.281 32/250 [==>...........................] - ETA: 1s - loss: 1.9789 - acc: 0.2638 - ETA: 1s - loss: 1.9156 - acc: 0.292 - ETA: 1s - loss: 1.9883 - acc: 0.268 - ETA: 1s - loss: 1.8973 - acc: 0.31 - ETA: 1s - loss: 1.9790 - acc: 0.2624 - ETA: 1s - loss: 1.9054 - acc: 0.291 - ETA: 1s - loss: 1.9750 - acc: 0.266 - ETA: 1s - loss: 1.8818 - acc: 0.32 - ETA: 1s - loss: 1.8991 - acc: 0.3012 - ETA: 1s - loss: 1.9779 - acc: 0.271 - ETA: 1s - loss: 1.9666 - acc: 0.269 - ETA: 1s - loss: 1.8608 - acc: 0.328 - ETA: 1s - loss: 1.9750 - acc: 0.272 - ETA: 1s - loss: 1.8797 - acc: 0.31 - ETA: 1s - loss: 1.9695 - acc: 0.2633 - ETA: 1s - loss: 1.8515 - acc: 0.33 59/250 [======>.......................] 65/250 [======>.......................] - ETA: 1s - loss: 1.8868 - acc: 0.3126 - ETA: 1s - loss: 1.9538 - acc: 0.277 - ETA: 1s - loss: 1.8528 - acc: 0.335 - ETA: 1s - loss: 1.9810 - acc: 0.261 - ETA: 1s - loss: 1.8753 - acc: 0.318 - ETA: 1s - loss: 1.9520 - acc: 0.276 - ETA: 1s - loss: 1.8377 - acc: 0.337 - ETA: 1s - loss: 1.9734 - acc: 0.26 76/250 [========>.....................] - ETA: 1s - loss: 1.9430 - acc: 0.2784 81/250 [========>.....................] - ETA: 1s - loss: 1.8639 - acc: 0.321 - ETA: 1s - loss: 1.8260 - acc: 0.342 - ETA: 1s - loss: 1.9587 - acc: 0.271 89/250 [=========>....................] - ETA: 1s - loss: 1.8630 - acc: 0.3272 - ETA: 1s - loss: 1.9437 - acc: 0.281 - ETA: 1s - loss: 1.8114 - acc: 0.348 - ETA: 1s - loss: 1.9480 - acc: 0.27 92/250 [==========>...................] 97/250 [==========>...................] - ETA: 1s - loss: 1.8495 - acc: 0.3322 - ETA: 1s - loss: 1.9480 - acc: 0.281 - ETA: 1s - loss: 1.8013 - acc: 0.350 - ETA: 1s - loss: 1.9433 - acc: 0.27100/250 [===========>..................] - ETA: 0s - loss: 1.9403 - acc: 0.285 - ETA: 0s - loss: 1.7886 - acc: 0.353 - ETA: 0s - loss: 1.9316 - acc: 0.28 - ETA: 0s - loss: 1.8429 - acc: 0.3386 - ETA: 0s - loss: 1.9370 - acc: 0.288 - ETA: 0s - loss: 1.7703 - acc: 0.363 - ETA: 0s - loss: 1.9280 - acc: 0.28 - ETA: 0s - loss: 1.9245 - acc: 0.2952 - ETA: 0s - loss: 1.8511 - acc: 0.344 - ETA: 0s - loss: 1.7588 - acc: 0.369 - ETA: 0s - loss: 1.9201 - acc: 0.29 - ETA: 0s - loss: 1.8425 - acc: 0.3486 - ETA: 0s - loss: 1.9194 - acc: 0.299 - ETA: 0s - loss: 1.7521 - acc: 0.37 - ETA: 0s - loss: 1.9112 - acc: 0.2949 - ETA: 0s - loss: 1.8389 - acc: 0.352 - ETA: 0s - loss: 1.9169 - acc: 0.300 - ETA: 0s - loss: 1.7421 - acc: 0.37 - ETA: 0s - loss: 1.9041 - acc: 0.2989 - ETA: 0s - loss: 1.8297 - acc: 0.357 - ETA: 0s - loss: 1.9098 - acc: 0.304 - ETA: 0s - loss: 1.7280 - acc: 0.38 - ETA: 0s - loss: 1.8116 - acc: 0.3643 - ETA: 0s - loss: 1.8966 - acc: 0.301 - ETA: 0s - loss: 1.9008 - acc: 0.307 - ETA: 0s - loss: 1.7158 - acc: 0.393 - ETA: 0s - loss: 1.8893 - acc: 0.3055162/250 [==================>...........] - ETA: 0s - loss: 1.8026 - acc: 0.367 - ETA: 0s - loss: 1.8998 - acc: 0.308 - ETA: 0s - loss: 1.7037 - acc: 0.398 - ETA: 0s - loss: 1.7868 - acc: 0.373 - ETA: 0s - loss: 1.8787 - acc: 0.310 - ETA: 0s - loss: 1.8940 - acc: 0.310 - ETA: 0s - loss: 1.6993 - acc: 0.40166/250 [==================>...........]178/250 [====================>.........] - ETA: 0s - loss: 1.7730 - acc: 0.3799 - ETA: 0s - loss: 1.8682 - acc: 0.316 - ETA: 0s - loss: 1.8873 - acc: 0.313 - ETA: 0s - loss: 1.6855 - acc: 0.406187/250 [=====================>........] - ETA: 0s - loss: 1.7617 - acc: 0.3868 - ETA: 0s - loss: 1.8602 - acc: 0.318 - ETA: 0s - loss: 1.8785 - acc: 0.317 - ETA: 0s - loss: 1.6779 - acc: 0.410 - ETA: 0s - loss: 1.7567 - acc: 0.388 - ETA: 0s - loss: 1.8538 - acc: 0.322 - ETA: 0s - loss: 1.8693 - acc: 0.321 - ETA: 0s - loss: 1.6680 - acc: 0.414 - ETA: 0s - loss: 1.7442 - acc: 0.394 - ETA: 0s - loss: 1.8437 - acc: 0.327 - ETA: 0s - loss: 1.8566 - acc: 0.327 - ETA: 0s - loss: 1.6558 - acc: 0.41198/250 [======================>.......] - ETA: 0s - loss: 1.8344 - acc: 0.331 - ETA: 0s - loss: 1.8476 - acc: 0.330 - ETA: 0s - loss: 1.6451 - acc: 0.425 - ETA: 0s - loss: 1.7239 - acc: 0.402 - ETA: 0s - loss: 1.8287 - acc: 0.335 - ETA: 0s - loss: 1.8368 - acc: 0.335 - ETA: 0s - loss: 1.6415 - acc: 0.429 - ETA: 0s - loss: 1.7100 - acc: 0.407 - ETA: 0s - loss: 1.8202 - acc: 0.339 - ETA: 0s - loss: 1.8264 - acc: 0.339 - ETA: 0s - loss: 1.6348 - acc: 0.43223/250 [=========================>....] - ETA: 0s - loss: 1.8114 - acc: 0.344 - ETA: 0s - loss: 1.8205 - acc: 0.343 - ETA: 0s - loss: 1.6249 - acc: 0.436 - ETA: 0s - loss: 1.6933 - acc: 0.414 - ETA: 0s - loss: 1.8043 - acc: 0.348 - ETA: 0s - loss: 1.8106 - acc: 0.34 - ETA: 0s - loss: 1.6145 - acc: 0.4410 - 2s 6ms/step - loss: 1.6806 - acc: 0.4195\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.7956 - acc: 0.352 - 2s 6ms/step - loss: 1.8085 - acc: 0.3518\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.6079 - acc: 0.444 - ETA: 0s - loss: 1.7854 - acc: 0.357 - 2s 6ms/step - loss: 1.6058 - acc: 0.4457\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.7837 - acc: 0.3582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 1.2564 - acc: 0.5000 - ETA: 2s - loss: 1.5769 - acc: 0.540 - ETA: 1s - loss: 1.2504 - acc: 0.5700 - ETA: 1s - loss: 1.5950 - acc: 0.4400 - ETA: 2s - loss: 1.4530 - acc: 0.480 - ETA: 1s - loss: 1.5636 - acc: 0.487 - ETA: 1s - loss: 1.2674 - acc: 0.584 - ETA: 1s - loss: 1.2412 - acc: 0.55 16/250 [>.............................]  8/250 [..............................] - ETA: 1s - loss: 1.5112 - acc: 0.4913 - ETA: 1s - loss: 1.5144 - acc: 0.485 - ETA: 1s - loss: 1.3169 - acc: 0.555 - ETA: 1s - loss: 1.2941 - acc: 0.561 25/250 [==>...........................] 17/250 [=>............................] - ETA: 1s - loss: 1.4462 - acc: 0.515 - ETA: 1s - loss: 1.3437 - acc: 0.54 - ETA: 1s - loss: 1.3024 - acc: 0.564 - ETA: 1s - loss: 1.4320 - acc: 0.5160 - ETA: 1s - loss: 1.4701 - acc: 0.506 - ETA: 1s - loss: 1.3271 - acc: 0.55 - ETA: 1s - loss: 1.3049 - acc: 0.5636 - ETA: 1s - loss: 1.4668 - acc: 0.509 - ETA: 1s - loss: 1.4278 - acc: 0.517 - ETA: 1s - loss: 1.3098 - acc: 0.562 41/250 [===>..........................] - ETA: 1s - loss: 1.4652 - acc: 0.5079 - ETA: 1s - loss: 1.3507 - acc: 0.554 - ETA: 1s - loss: 1.4386 - acc: 0.516 - ETA: 1s - loss: 1.3197 - acc: 0.557 - ETA: 1s - loss: 1.3512 - acc: 0.556 - ETA: 1s - loss: 1.4795 - acc: 0.506 - ETA: 1s - loss: 1.4296 - acc: 0.521 - ETA: 1s - loss: 1.3276 - acc: 0.553 - ETA: 1s - loss: 1.3508 - acc: 0.556 - ETA: 1s - loss: 1.4683 - acc: 0.510 - ETA: 1s - loss: 1.4150 - acc: 0.526 - ETA: 1s - loss: 1.3312 - acc: 0.553 - ETA: 1s - loss: 1.3420 - acc: 0.55 - ETA: 1s - loss: 1.4567 - acc: 0.5146 - ETA: 1s - loss: 1.4180 - acc: 0.526 - ETA: 1s - loss: 1.3363 - acc: 0.551 74/250 [=======>......................] - ETA: 1s - loss: 1.3310 - acc: 0.560 75/250 [========>.....................] - ETA: 1s - loss: 1.4116 - acc: 0.530 - ETA: 1s - loss: 1.3334 - acc: 0.551 - ETA: 1s - loss: 1.4333 - acc: 0.5204 82/250 [========>.....................] - ETA: 1s - loss: 1.3325 - acc: 0.563 - ETA: 1s - loss: 1.4114 - acc: 0.531 - ETA: 1s - loss: 1.3272 - acc: 0.55 99/250 [==========>...................] - ETA: 0s - loss: 1.4393 - acc: 0.519 - ETA: 1s - loss: 1.4136 - acc: 0.530 - ETA: 0s - loss: 1.3309 - acc: 0.557 99/250 [==========>...................] - ETA: 0s - loss: 1.4311 - acc: 0.521 - ETA: 0s - loss: 1.3069 - acc: 0.5705 - ETA: 0s - loss: 1.4076 - acc: 0.533 - ETA: 0s - loss: 1.3297 - acc: 0.55107/250 [===========>..................] - ETA: 0s - loss: 1.3082 - acc: 0.5710115/250 [============>.................] - ETA: 0s - loss: 1.4337 - acc: 0.520 - ETA: 0s - loss: 1.4042 - acc: 0.534 - ETA: 0s - loss: 1.3243 - acc: 0.561 - ETA: 0s - loss: 1.4274 - acc: 0.52 - ETA: 0s - loss: 1.3125 - acc: 0.5720 - ETA: 0s - loss: 1.3979 - acc: 0.536 - ETA: 0s - loss: 1.3192 - acc: 0.561 - ETA: 0s - loss: 1.4294 - acc: 0.521 - ETA: 0s - loss: 1.3114 - acc: 0.572 - ETA: 0s - loss: 1.3881 - acc: 0.541 - ETA: 0s - loss: 1.3204 - acc: 0.56130/250 [==============>...............] - ETA: 0s - loss: 1.4189 - acc: 0.5252 - ETA: 0s - loss: 1.3146 - acc: 0.5698 - ETA: 0s - loss: 1.3828 - acc: 0.543 - ETA: 0s - loss: 1.3126 - acc: 0.56 - ETA: 0s - loss: 1.4075 - acc: 0.5291 - ETA: 0s - loss: 1.3168 - acc: 0.568 - ETA: 0s - loss: 1.3794 - acc: 0.544 - ETA: 0s - loss: 1.3108 - acc: 0.564157/250 [=================>............] - ETA: 0s - loss: 1.3775 - acc: 0.5455 - ETA: 0s - loss: 1.4087 - acc: 0.5331148/250 [================>.............] - ETA: 0s - loss: 1.3158 - acc: 0.568 - ETA: 0s - loss: 1.3044 - acc: 0.567156/250 [=================>............] - ETA: 0s - loss: 1.3122 - acc: 0.5700 - ETA: 0s - loss: 1.4020 - acc: 0.533 - ETA: 0s - loss: 1.3707 - acc: 0.544 - ETA: 0s - loss: 1.2968 - acc: 0.56 - ETA: 0s - loss: 1.3105 - acc: 0.5718 - ETA: 0s - loss: 1.3966 - acc: 0.536 - ETA: 0s - loss: 1.3712 - acc: 0.544 - ETA: 0s - loss: 1.2962 - acc: 0.57 - ETA: 0s - loss: 1.3114 - acc: 0.572182/250 [====================>.........] - ETA: 0s - loss: 1.3896 - acc: 0.540 - ETA: 0s - loss: 1.2956 - acc: 0.571181/250 [====================>.........] - ETA: 0s - loss: 1.3127 - acc: 0.5717 - ETA: 0s - loss: 1.3571 - acc: 0.550 - ETA: 0s - loss: 1.3900 - acc: 0.542 - ETA: 0s - loss: 1.2896 - acc: 0.57189/250 [=====================>........]191/250 [=====================>........] - ETA: 0s - loss: 1.3583 - acc: 0.5511 - ETA: 0s - loss: 1.3030 - acc: 0.574 - ETA: 0s - loss: 1.3870 - acc: 0.543 - ETA: 0s - loss: 1.2896 - acc: 0.574 - ETA: 0s - loss: 1.3013 - acc: 0.576 - ETA: 0s - loss: 1.3535 - acc: 0.551 - ETA: 0s - loss: 1.3847 - acc: 0.544 - ETA: 0s - loss: 1.2853 - acc: 0.575206/250 [=======================>......] - ETA: 0s - loss: 1.2924 - acc: 0.5796 - ETA: 0s - loss: 1.3471 - acc: 0.552 - ETA: 0s - loss: 1.3810 - acc: 0.545 - ETA: 0s - loss: 1.2849 - acc: 0.575214/250 [========================>.....] - ETA: 0s - loss: 1.2878 - acc: 0.5812 - ETA: 0s - loss: 1.3465 - acc: 0.553 - ETA: 0s - loss: 1.3781 - acc: 0.545 - ETA: 0s - loss: 1.2881 - acc: 0.57224/250 [=========================>....] - ETA: 0s - loss: 1.3427 - acc: 0.554 - ETA: 0s - loss: 1.3775 - acc: 0.546 - ETA: 0s - loss: 1.2864 - acc: 0.57233/250 [==========================>...]230/250 [==========================>...] - ETA: 0s - loss: 1.2772 - acc: 0.5831 - ETA: 0s - loss: 1.3389 - acc: 0.556 - ETA: 0s - loss: 1.3781 - acc: 0.546 - ETA: 0s - loss: 1.2916 - acc: 0.573 - 2s 7ms/step - loss: 1.2889 - acc: 0.5745\n",
      "250/250 [==============================]]237/250 [===========================>..] - ETA: 0s - loss: 1.2790 - acc: 0.583 - ETA: 0s - loss: 1.3752 - acc: 0.546 - 2s 6ms/step - loss: 1.3751 - acc: 0.5468\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.3338 - acc: 0.5588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247/250 [============================>.] - ETA: 0s - loss: 1.2769 - acc: 0.5826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 7ms/step - loss: 1.2774 - acc: 0.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Received) round: b'3:0', device: 0\n",
      "(Received) round: b'3:1', device: 1\n",
      "(Received) round: b'3:2', device: 2\n",
      "(Received) round: b'3:3', device: 3\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 5:45 - loss: 1.0969 - acc: 0.6800 - ETA: 5:50 - loss: 1.1109 - acc: 0.6000 - ETA: 5:49 - loss: 1.1114 - acc: 0.6400 - ETA: 5:51 - loss: 0.9548 - acc: 0.700 - ETA: 28s - loss: 1.1203 - acc: 0.6150 - ETA: 31s - loss: 1.0396 - acc: 0.670 - ETA: 26s - loss: 1.0773 - acc: 0.6415  - ETA: 31s - loss: 1.0486 - acc: 0.636 - ETA: 15s - loss: 1.0959 - acc: 0.63 - ETA: 17s - loss: 1.0918 - acc: 0. - ETA: 16s - loss: 1.1237 - acc: 0.6276 - ETA: 14s - loss: 1.1062 - acc: 0.64 - ETA: 10s - loss: 1.0774 - acc: 0.64 - ETA: 10s - loss: 1.1177 - acc: 0.64 - ETA: 10s - loss: 1.1120 - acc: 0.62 - ETA: 11s - loss: 1.1614 - acc: 0.60 - ETA: 8s - loss: 1.0923 - acc: 0.636 - ETA: 8s - loss: 1.0997 - acc: 0.640 - ETA: 8s - loss: 1.1109 - acc: 0.623 - ETA: 8s - loss: 1.1453 - acc: 0.6160 - ETA: 6s - loss: 1.1157 - acc: 0.634 - ETA: 6s - loss: 1.1059 - acc: 0.631 - ETA: 7s - loss: 1.1212 - acc: 0.625 - ETA: 7s - loss: 1.1245 - acc: 0.629 - ETA: 5s - loss: 1.1143 - acc: 0.637 - ETA: 5s - loss: 1.1194 - acc: 0.62 56/250 [=====>........................] 57/250 [=====>........................] - ETA: 5s - loss: 1.1266 - acc: 0.6329 - ETA: 5s - loss: 1.1122 - acc: 0.634 - ETA: 4s - loss: 1.1284 - acc: 0.636 - ETA: 5s - loss: 1.1317 - acc: 0.62 65/250 [======>.......................] - ETA: 5s - loss: 1.1117 - acc: 0.636 - ETA: 4s - loss: 1.1288 - acc: 0.633 - ETA: 4s - loss: 1.1230 - acc: 0.630 73/250 [=======>......................] - ETA: 4s - loss: 1.1028 - acc: 0.6397 - ETA: 4s - loss: 1.1385 - acc: 0.632 - ETA: 3s - loss: 1.1319 - acc: 0.632 - ETA: 3s - loss: 1.1238 - acc: 0.632 - ETA: 3s - loss: 1.1313 - acc: 0.634 - ETA: 3s - loss: 1.1115 - acc: 0.637 - ETA: 3s - loss: 1.1288 - acc: 0.631 - ETA: 3s - loss: 1.1258 - acc: 0.631 - ETA: 3s - loss: 1.1327 - acc: 0.633 - ETA: 3s - loss: 1.1145 - acc: 0.636 - ETA: 3s - loss: 1.1204 - acc: 0.633 - ETA: 3s - loss: 1.1343 - acc: 0.630 - ETA: 3s - loss: 1.1286 - acc: 0.634 - ETA: 3s - loss: 1.1221 - acc: 0.632 - ETA: 2s - loss: 1.1115 - acc: 0.636 - ETA: 2s - loss: 1.1377 - acc: 0.62 - ETA: 2s - loss: 1.1316 - acc: 0.6338 - ETA: 2s - loss: 1.1254 - acc: 0.633 - ETA: 2s - loss: 1.1125 - acc: 0.636 - ETA: 2s - loss: 1.1348 - acc: 0.630 - ETA: 2s - loss: 1.1320 - acc: 0.631 - ETA: 2s - loss: 1.1288 - acc: 0.631 - ETA: 2s - loss: 1.1179 - acc: 0.63 - ETA: 2s - loss: 1.1279 - acc: 0.6312 - ETA: 2s - loss: 1.1350 - acc: 0.630 - ETA: 2s - loss: 1.1231 - acc: 0.630 - ETA: 1s - loss: 1.1156 - acc: 0.63 - ETA: 2s - loss: 1.1365 - acc: 0.6296 - ETA: 2s - loss: 1.1366 - acc: 0.629 - ETA: 1s - loss: 1.1235 - acc: 0.629 - ETA: 1s - loss: 1.1192 - acc: 0.630 - ETA: 1s - loss: 1.1375 - acc: 0.630 - ETA: 1s - loss: 1.1347 - acc: 0.631 - ETA: 1s - loss: 1.1198 - acc: 0.633 - ETA: 1s - loss: 1.1203 - acc: 0.63145/250 [================>.............] - ETA: 1s - loss: 1.1400 - acc: 0.631 - ETA: 1s - loss: 1.1262 - acc: 0.631 - ETA: 1s - loss: 1.1220 - acc: 0.63153/250 [=================>............] - ETA: 1s - loss: 1.1388 - acc: 0.6315155/250 [=================>............] - ETA: 1s - loss: 1.1406 - acc: 0.631 - ETA: 1s - loss: 1.1376 - acc: 0.628 - ETA: 1s - loss: 1.1254 - acc: 0.630161/250 [==================>...........] - ETA: 1s - loss: 1.1338 - acc: 0.6334 - ETA: 1s - loss: 1.1395 - acc: 0.630 - ETA: 1s - loss: 1.1393 - acc: 0.627 - ETA: 1s - loss: 1.1287 - acc: 0.630 - ETA: 1s - loss: 1.1411 - acc: 0.630 - ETA: 1s - loss: 1.1294 - acc: 0.63 - ETA: 0s - loss: 1.1338 - acc: 0.6290 - ETA: 1s - loss: 1.1447 - acc: 0.625 - ETA: 0s - loss: 1.1424 - acc: 0.631 - ETA: 0s - loss: 1.1352 - acc: 0.634 - ETA: 0s - loss: 1.1399 - acc: 0.625 - ETA: 0s - loss: 1.1346 - acc: 0.628 - ETA: 0s - loss: 1.1465 - acc: 0.629 - ETA: 0s - loss: 1.1379 - acc: 0.631 - ETA: 0s - loss: 1.1376 - acc: 0.626 - ETA: 0s - loss: 1.1367 - acc: 0.628 - ETA: 0s - loss: 1.1446 - acc: 0.629 - ETA: 0s - loss: 1.1381 - acc: 0.631 - ETA: 0s - loss: 1.1408 - acc: 0.626 - ETA: 0s - loss: 1.1412 - acc: 0.62 - ETA: 0s - loss: 1.1453 - acc: 0.6293 - ETA: 0s - loss: 1.1428 - acc: 0.629 - ETA: 0s - loss: 1.1426 - acc: 0.624 - ETA: 0s - loss: 1.1428 - acc: 0.624 - ETA: 0s - loss: 1.1463 - acc: 0.629 - ETA: 0s - loss: 1.1422 - acc: 0.629 - ETA: 0s - loss: 1.1469 - acc: 0.623 - ETA: 0s - loss: 1.1492 - acc: 0.623 - ETA: 0s - loss: 1.1467 - acc: 0.629 - ETA: 0s - loss: 1.1405 - acc: 0.630 - ETA: 0s - loss: 1.1448 - acc: 0.624 - ETA: 0s - loss: 1.1506 - acc: 0.623 - ETA: 0s - loss: 1.1424 - acc: 0.62230/250 [==========================>...] - ETA: 0s - loss: 1.1464 - acc: 0.623 - ETA: 0s - loss: 1.1484 - acc: 0.623 - ETA: 0s - loss: 1.1426 - acc: 0.629 - ETA: 0s - loss: 1.1405 - acc: 0.629 - ETA: 0s - loss: 1.1477 - acc: 0.623 - ETA: 0s - loss: 1.1507 - acc: 0.621 - ETA: 0s - loss: 1.1444 - acc: 0.630 - 3s 12ms/step - loss: 1.1520 - acc: 0.6210\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.1399 - acc: 0.6289246/250 [============================>.] - ETA: 0s - loss: 1.1494 - acc: 0.622 - 3s 12ms/step - loss: 1.1441 - acc: 0.6306\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 1.1407 - acc: 0.6284\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 1.1471 - acc: 0.6235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 1.0774 - acc: 0.6800 - ETA: 2s - loss: 1.4383 - acc: 0.640  1/250 [..............................]  1/250 [..............................] - ETA: 2s - loss: 1.0991 - acc: 0.6600  9/250 [>.............................] - ETA: 1s - loss: 1.0898 - acc: 0.640 - ETA: 1s - loss: 1.1987 - acc: 0.646 - ETA: 2s - loss: 1.1541 - acc: 0.610 17/250 [=>............................]  9/250 [>.............................] - ETA: 1s - loss: 1.1206 - acc: 0.628 - ETA: 1s - loss: 1.2238 - acc: 0.617 17/250 [=>............................] - ETA: 1s - loss: 1.1388 - acc: 0.614 - ETA: 1s - loss: 1.1208 - acc: 0.6440 - ETA: 1s - loss: 1.0985 - acc: 0.630 - ETA: 1s - loss: 1.1857 - acc: 0.61 - ETA: 1s - loss: 1.1143 - acc: 0.6256 - ETA: 1s - loss: 1.1171 - acc: 0.641 - ETA: 1s - loss: 1.1234 - acc: 0.629 - ETA: 1s - loss: 1.1934 - acc: 0.610 - ETA: 1s - loss: 1.1054 - acc: 0.647 - ETA: 1s - loss: 1.1251 - acc: 0.6282 34/250 [===>..........................] - ETA: 1s - loss: 1.1133 - acc: 0.619 - ETA: 1s - loss: 1.1788 - acc: 0.621 50/250 [=====>........................] - ETA: 1s - loss: 1.1208 - acc: 0.637 - ETA: 1s - loss: 1.1119 - acc: 0.6302 - ETA: 1s - loss: 1.1362 - acc: 0.621 - ETA: 1s - loss: 1.1679 - acc: 0.62 51/250 [=====>........................] - ETA: 1s - loss: 1.1444 - acc: 0.614 - ETA: 1s - loss: 1.1335 - acc: 0.6325 58/250 [=====>........................] - ETA: 1s - loss: 1.1253 - acc: 0.637 - ETA: 1s - loss: 1.1536 - acc: 0.62 59/250 [======>....................... - ETA: 1s - loss: 1.1213 - acc: 0.6373 - ETA: 1s - loss: 1.1256 - acc: 0.637 - ETA: 1s - loss: 1.1729 - acc: 0.62 - ETA: 1s - loss: 1.1339 - acc: 0.6191 75/250 [========>.....................] 67/250 [=======>......................] - ETA: 1s - loss: 1.1174 - acc: 0.636 - ETA: 1s - loss: 1.1748 - acc: 0.62 75/250 [========>.....................] - ETA: 1s - loss: 1.1171 - acc: 0.624 83/250 [========>.....................] - ETA: 1s - loss: 1.1042 - acc: 0.6436 - ETA: 1s - loss: 1.1069 - acc: 0.638 - ETA: 1s - loss: 1.1883 - acc: 0.61 91/250 [=========>....................] 83/250 [========>.....................] - ETA: 1s - loss: 1.1205 - acc: 0.6212 - ETA: 1s - loss: 1.1009 - acc: 0.642 - ETA: 1s - loss: 1.1205 - acc: 0.637 - ETA: 1s - loss: 1.1812 - acc: 0.619 98/250 [==========>...................] - ETA: 0s - loss: 1.1086 - acc: 0.6402 - ETA: 1s - loss: 1.1255 - acc: 0.620 - ETA: 1s - loss: 1.1249 - acc: 0.636 - ETA: 0s - loss: 1.1736 - acc: 0.622 - ETA: 0s - loss: 1.1024 - acc: 0.642 - ETA: 0s - loss: 1.1249 - acc: 0.6350101/250 [===========>..................] - ETA: 0s - loss: 1.1256 - acc: 0.620 - ETA: 0s - loss: 1.1677 - acc: 0.625 - ETA: 0s - loss: 1.1219 - acc: 0.62109/250 [============>.................] - ETA: 0s - loss: 1.1232 - acc: 0.638 - ETA: 0s - loss: 1.1568 - acc: 0.62124/250 [=============>................] - ETA: 0s - loss: 1.1049 - acc: 0.640 - ETA: 0s - loss: 1.1290 - acc: 0.637 - ETA: 0s - loss: 1.1544 - acc: 0.62 - ETA: 0s - loss: 1.1193 - acc: 0.6251133/250 [==============>...............] - ETA: 0s - loss: 1.1045 - acc: 0.6411 - ETA: 0s - loss: 1.1260 - acc: 0.639 - ETA: 0s - loss: 1.1456 - acc: 0.629 - ETA: 0s - loss: 1.1204 - acc: 0.62 - ETA: 0s - loss: 1.1125 - acc: 0.6365 - ETA: 0s - loss: 1.1304 - acc: 0.638 - ETA: 0s - loss: 1.1450 - acc: 0.630149/250 [================>.............]141/250 [===============>..............] - ETA: 0s - loss: 1.1116 - acc: 0.6368 - ETA: 0s - loss: 1.1330 - acc: 0.6367 - ETA: 0s - loss: 1.1126 - acc: 0.628 - ETA: 0s - loss: 1.1468 - acc: 0.6150/250 [=================>............] - ETA: 0s - loss: 1.1241 - acc: 0.6392 - ETA: 0s - loss: 1.1115 - acc: 0.6298158/250 [=================>............] - ETA: 0s - loss: 1.1146 - acc: 0.636 - ETA: 0s - loss: 1.1428 - acc: 0.631157/250 [=================>............]158/250 [=================>............] - ETA: 0s - loss: 1.1279 - acc: 0.6372 - ETA: 0s - loss: 1.1150 - acc: 0.6293 - ETA: 0s - loss: 1.1170 - acc: 0.635 - ETA: 0s - loss: 1.1425 - acc: 0.632 - ETA: 0s - loss: 1.1167 - acc: 0.63 - ETA: 0s - loss: 1.1290 - acc: 0.6366167/250 [===================>.......... - ETA: 0s - loss: 1.1358 - acc: 0.63183/250 [====================>.........] - ETA: 0s - loss: 1.1234 - acc: 0.633174/250 [===================>..........] - ETA: 0s - loss: 1.1130 - acc: 0.632 - ETA: 0s - loss: 1.1331 - acc: 0.6352 - ETA: 0s - loss: 1.1316 - acc: 0.633 - ETA: 0s - loss: 1.1102 - acc: 0.63 - ETA: 0s - loss: 1.1250 - acc: 0.6336 - ETA: 0s - loss: 1.1335 - acc: 0.635 - ETA: 0s - loss: 1.1298 - acc: 0.632 - ETA: 0s - loss: 1.1099 - acc: 0.632 - ETA: 0s - loss: 1.1275 - acc: 0.632 - ETA: 0s - loss: 1.1279 - acc: 0.636 - ETA: 0s - loss: 1.1281 - acc: 0.63 - ETA: 0s - loss: 1.1099 - acc: 0.632 - ETA: 0s - loss: 1.1285 - acc: 0.6321 - ETA: 0s - loss: 1.1356 - acc: 0.633 - ETA: 0s - loss: 1.1326 - acc: 0.6 - ETA: 0s - loss: 1.1088 - acc: 0.6337209/250 [========================>.....] - ETA: 0s - loss: 1.1368 - acc: 0.633 - ETA: 0s - loss: 1.1322 - acc: 0.632215/250 [========================>.....] - ETA: 0s - loss: 1.1069 - acc: 0.634218/250 [=========================>....]224/250 [=========================>....] - ETA: 0s - loss: 1.1304 - acc: 0.6341 - ETA: 0s - loss: 1.1243 - acc: 0.632 - ETA: 0s - loss: 1.1341 - acc: 0.632232/250 [==========================>...] - ETA: 0s - loss: 1.1229 - acc: 0.6323 - ETA: 0s - loss: 1.1002 - acc: 0.635 - ETA: 0s - loss: 1.1286 - acc: 0.634 - ETA: 0s - loss: 1.1316 - acc: 0.6 - ETA: 0s - loss: 1.1255 - acc: 0.633 - ETA: 0s - loss: 1.1305 - acc: 0.6342234/250 [===========================>..] - ETA: 0s - loss: 1.1283 - acc: 0.634 - ETA: 0s - loss: 1.1259 - acc: 0.6336238/250 [===========================>..] - ETA: 0s - loss: 1.1042 - acc: 0.63 - 2s 6ms/step - loss: 1.1251 - acc: 0.6342\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.1285 - acc: 0.634 - 2s 6ms/step - loss: 1.1286 - acc: 0.6346\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1271 - acc: 0.6346\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1045 - acc: 0.6356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 1.3968 - acc: 0.5600 - ETA: 2s - loss: 1.1326 - acc: 0.6600  1/250 [..............................] - ETA: 1s - loss: 1.2275 - acc: 0.5822 - ETA: 1s - loss: 0.9496 - acc: 0.640 - ETA: 1s - loss: 1.0454 - acc: 0.6400 - ETA: 1s - loss: 1.2471 - acc: 0.640 - ETA: 1s - loss: 1.1553 - acc: 0.632 - ETA: 1s - loss: 1.1562 - acc: 0.601  9/250 [>.............................] - ETA: 1s - loss: 1.0727 - acc: 0.6459 - ETA: 1s - loss: 1.1233 - acc: 0.633 - ETA: 1s - loss: 1.1121 - acc: 0.627 - ETA: 1s - loss: 1.1354 - acc: 0.61 - ETA: 1s - loss: 1.1058 - acc: 0.6400 - ETA: 1s - loss: 1.0755 - acc: 0.651 - ETA: 1s - loss: 1.1544 - acc: 0.618 - ETA: 1s - loss: 1.1563 - acc: 0.61 - ETA: 1s - loss: 1.1492 - acc: 0.6346 - ETA: 1s - loss: 1.1013 - acc: 0.644 - ETA: 1s - loss: 1.1208 - acc: 0.636 - ETA: 1s - loss: 1.1542 - acc: 0.615 34/250 [===>..........................] 41/250 [===>..........................] - ETA: 1s - loss: 1.0964 - acc: 0.640 - ETA: 1s - loss: 1.0962 - acc: 0.642 - ETA: 1s - loss: 1.1411 - acc: 0.619 - ETA: 1s - loss: 1.1549 - acc: 0.624 - ETA: 1s - loss: 1.0916 - acc: 0.634 - ETA: 1s - loss: 1.0819 - acc: 0.647 - ETA: 1s - loss: 1.1348 - acc: 0.621 - ETA: 1s - loss: 1.1452 - acc: 0.6284 59/250 [======>.......................] - ETA: 1s - loss: 1.1015 - acc: 0.628 - ETA: 1s - loss: 1.0701 - acc: 0.656 - ETA: 1s - loss: 1.1353 - acc: 0.623 57/250 [=====>........................] - ETA: 1s - loss: 1.1391 - acc: 0.6358 - ETA: 1s - loss: 1.0999 - acc: 0.629 - ETA: 1s - loss: 1.0842 - acc: 0.655 - ETA: 1s - loss: 1.1256 - acc: 0.63 66/250 [======>.......................] 75/250 [========>.....................] - ETA: 1s - loss: 1.1267 - acc: 0.6370 - ETA: 1s - loss: 1.1030 - acc: 0.630 - ETA: 1s - loss: 1.0676 - acc: 0.661 - ETA: 1s - loss: 1.1209 - acc: 0.632 - ETA: 1s - loss: 1.1090 - acc: 0.630 - ETA: 1s - loss: 1.1308 - acc: 0.634 - ETA: 1s - loss: 1.0748 - acc: 0.65 - ETA: 1s - loss: 1.1079 - acc: 0.6352 - ETA: 1s - loss: 1.1085 - acc: 0.631 - ETA: 1s - loss: 1.1253 - acc: 0.636 - ETA: 1s - loss: 1.0706 - acc: 0.656 - ETA: 0s - loss: 1.1085 - acc: 0.635 - ETA: 0s - loss: 1.1117 - acc: 0.630 - ETA: 1s - loss: 1.1298 - acc: 0.635 - ETA: 0s - loss: 1.0752 - acc: 0.655 - ETA: 0s - loss: 1.1099 - acc: 0.635 - ETA: 0s - loss: 1.1116 - acc: 0.628 - ETA: 0s - loss: 1.1247 - acc: 0.635 - ETA: 0s - loss: 1.0706 - acc: 0.655 - ETA: 0s - loss: 1.1106 - acc: 0.635 - ETA: 0s - loss: 1.1193 - acc: 0.625 - ETA: 0s - loss: 1.1271 - acc: 0.638 - ETA: 0s - loss: 1.0699 - acc: 0.655 - ETA: 0s - loss: 1.1101 - acc: 0.636 - ETA: 0s - loss: 1.1243 - acc: 0.626 - ETA: 0s - loss: 1.1313 - acc: 0.635 - ETA: 0s - loss: 1.0732 - acc: 0.654 - ETA: 0s - loss: 1.1154 - acc: 0.63 - ETA: 0s - loss: 1.1209 - acc: 0.6285 - ETA: 0s - loss: 1.1283 - acc: 0.636 - ETA: 0s - loss: 1.0713 - acc: 0.654 - ETA: 0s - loss: 1.1151 - acc: 0.635 - ETA: 0s - loss: 1.1184 - acc: 0.630 - ETA: 0s - loss: 1.1280 - acc: 0.634 - ETA: 0s - loss: 1.0755 - acc: 0.652 - ETA: 0s - loss: 1.1123 - acc: 0.637 - ETA: 0s - loss: 1.1131 - acc: 0.633 - ETA: 0s - loss: 1.1288 - acc: 0.63157/250 [=================>............] - ETA: 0s - loss: 1.1152 - acc: 0.6377151/250 [=================>............] - ETA: 0s - loss: 1.0747 - acc: 0.652 - ETA: 0s - loss: 1.1094 - acc: 0.634 - ETA: 0s - loss: 1.1300 - acc: 0.63159/250 [==================>...........]165/250 [==================>...........] - ETA: 0s - loss: 1.0815 - acc: 0.6508 - ETA: 0s - loss: 1.1174 - acc: 0.63157/250 [=================>............] - ETA: 0s - loss: 1.1246 - acc: 0.6364163/250 [==================>...........] - ETA: 0s - loss: 1.1086 - acc: 0.634 - ETA: 0s - loss: 1.0894 - acc: 0.648 - ETA: 0s - loss: 1.1247 - acc: 0.636 - ETA: 0s - loss: 1.1023 - acc: 0.635 - ETA: 0s - loss: 1.1290 - acc: 0.63 - ETA: 0s - loss: 1.0855 - acc: 0.6481 - ETA: 0s - loss: 1.1257 - acc: 0.637 - ETA: 0s - loss: 1.1011 - acc: 0.635 - ETA: 0s - loss: 1.1309 - acc: 0.635 - ETA: 0s - loss: 1.1256 - acc: 0.637 - ETA: 0s - loss: 1.0906 - acc: 0.646 - ETA: 0s - loss: 1.1011 - acc: 0.634 - ETA: 0s - loss: 1.1291 - acc: 0.637 - ETA: 0s - loss: 1.1250 - acc: 0.636 - ETA: 0s - loss: 1.0954 - acc: 0.645 - ETA: 0s - loss: 1.1027 - acc: 0.634 - ETA: 0s - loss: 1.1325 - acc: 0.63201/250 [=======================>......] - ETA: 0s - loss: 1.0999 - acc: 0.6435205/250 [=======================>......] - ETA: 0s - loss: 1.1266 - acc: 0.63205/250 [=======================>......]197/250 [======================>.......] - ETA: 0s - loss: 1.1356 - acc: 0.6369 - ETA: 0s - loss: 1.1031 - acc: 0.633214/250 [========================>.....] - ETA: 0s - loss: 1.1021 - acc: 0.6425 - ETA: 0s - loss: 1.1263 - acc: 0.63 - ETA: 0s - loss: 1.1041 - acc: 0.6341 - ETA: 0s - loss: 1.1319 - acc: 0.637 - ETA: 0s - loss: 1.1024 - acc: 0.642 - ETA: 0s - loss: 1.1289 - acc: 0.63 - ETA: 0s - loss: 1.1351 - acc: 0.6363 - ETA: 0s - loss: 1.1038 - acc: 0.634 - ETA: 0s - loss: 1.1065 - acc: 0.641 - ETA: 0s - loss: 1.1298 - acc: 0.633 - ETA: 0s - loss: 1.1338 - acc: 0.636 - ETA: 0s - loss: 1.1012 - acc: 0.63 - ETA: 0s - loss: 1.1034 - acc: 0.6432 - ETA: 0s - loss: 1.1301 - acc: 0.63238/250 [===========================>..] - ETA: 0s - loss: 1.1001 - acc: 0.6362231/250 [==========================>...] - ETA: 0s - loss: 1.1320 - acc: 0.636 - ETA: 0s - loss: 1.1082 - acc: 0.641 - ETA: 0s - loss: 1.1290 - acc: 0.633239/250 [===========================>..] - ETA: 0s - loss: 1.1288 - acc: 0.6375 - ETA: 0s - loss: 1.0979 - acc: 0.637 - 2s 6ms/step - loss: 1.1285 - acc: 0.6331\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.0983 - acc: 0.6380\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1107 - acc: 0.6405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 1.1289 - acc: 0.637 - 2s 6ms/step - loss: 1.1288 - acc: 0.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Received) round: b'4:0', device: 0\n",
      "(Received) round: b'4:1', device: 1\n",
      "(Received) round: b'4:2', device: 2\n",
      "(Received) round: b'4:3', device: 3\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:22: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/Jeff/config/environment.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructorWARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "WARNING:tensorflow:From /home/mnetlab/anaconda3/envs/pp_fl/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - ETA: 0s - loss: 1.0429 - acc: 0.6586] - ETA: 5:41 - loss: 0.9780 - acc: 0.720 - ETA: 25s - loss: 1.0432 - acc: 0.6585  - ETA: 5:45 - loss: 1.1676 - acc: 0.6400 - ETA: 5:50 - loss: 1.2128 - acc: 0.6600 - ETA: 5:54 - loss: 1.0116 - acc: 0.6 - ETA: 26s - loss: 1.0583 - acc: 0.6600  - ETA: 14s - loss: 0.9995 - acc: 0.667 - ETA: 28s - loss: 0.8977 - acc: 0.7 10/250 [>.............................] - ETA: 35s - loss: 1.0109 - acc: 0.6840  33/250 [==>...........................] 25/250 [==>...........................] - ETA: 10s - loss: 0.9964 - acc: 0.6642 - ETA: 13s - loss: 1.0528 - acc: 0.65 - ETA: 15s - loss: 0.9142 - acc: 0.69 - ETA: 20s - loss: 1.0334 - acc: 0. - ETA: 7s - loss: 0.9907 - acc: 0.6684  - ETA: 10s - loss: 1.0273 - acc: 0.65 - ETA: 10s - loss: 0.9276 - acc: 0.68 - ETA: 12s - loss: 1.0420 - acc: 0.673 - ETA: 6s - loss: 0.9920 - acc: 0.67 - ETA: 7s - loss: 1.0201 - acc: 0.657 - ETA: 8s - loss: 0.9563 - acc: 0.678 - ETA: 8s - loss: 1.0403 - acc: 0.6697 - ETA: 5s - loss: 0.9764 - acc: 0.675 - ETA: 6s - loss: 1.0077 - acc: 0.660 - ETA: 6s - loss: 0.9723 - acc: 0.67 - ETA: 4s - loss: 0.9876 - acc: 0.6710 - ETA: 7s - loss: 1.0245 - acc: 0.669 - ETA: 5s - loss: 1.0017 - acc: 0.661 - ETA: 5s - loss: 0.9817 - acc: 0.676 - ETA: 6s - loss: 1.0083 - acc: 0.674 - ETA: 4s - loss: 0.9895 - acc: 0.668 - ETA: 4s - loss: 0.9991 - acc: 0.663 - ETA: 5s - loss: 0.9779 - acc: 0.676 - ETA: 5s - loss: 1.0092 - acc: 0.67 - ETA: 4s - loss: 0.9998 - acc: 0.6642 - ETA: 3s - loss: 0.9991 - acc: 0.669 - ETA: 4s - loss: 0.9755 - acc: 0.678 - ETA: 4s - loss: 1.0019 - acc: 0.677 - ETA: 3s - loss: 1.0082 - acc: 0.665 - ETA: 3s - loss: 1.0040 - acc: 0.663 - ETA: 3s - loss: 0.9806 - acc: 0.677 - ETA: 4s - loss: 1.0069 - acc: 0.67 89/250 [=========>....................] - ETA: 3s - loss: 1.0063 - acc: 0.6649101/250 [===========>..................] - ETA: 2s - loss: 1.0061 - acc: 0.667 - ETA: 3s - loss: 0.9805 - acc: 0.676 - ETA: 3s - loss: 1.0182 - acc: 0.669 - ETA: 2s - loss: 1.0046 - acc: 0.669 - ETA: 3s - loss: 1.0033 - acc: 0.667 - ETA: 3s - loss: 0.9782 - acc: 0.676 - ETA: 3s - loss: 1.0213 - acc: 0.67 - ETA: 2s - loss: 1.0084 - acc: 0.6685 - ETA: 2s - loss: 1.0124 - acc: 0.665 - ETA: 2s - loss: 0.9848 - acc: 0.675 - ETA: 2s - loss: 1.0153 - acc: 0.672 - ETA: 2s - loss: 1.0054 - acc: 0.669 - ETA: 2s - loss: 1.0216 - acc: 0.663 - ETA: 2s - loss: 0.9846 - acc: 0.672 - ETA: 2s - loss: 1.0201 - acc: 0.671 - ETA: 1s - loss: 1.0050 - acc: 0.668 - ETA: 2s - loss: 1.0225 - acc: 0.663 - ETA: 2s - loss: 0.9849 - acc: 0.671 - ETA: 2s - loss: 1.0249 - acc: 0.668 - ETA: 1s - loss: 1.0095 - acc: 0.666 - ETA: 2s - loss: 1.0278 - acc: 0.663 - ETA: 1s - loss: 0.9889 - acc: 0.671 - ETA: 2s - loss: 1.0286 - acc: 0.666 - ETA: 1s - loss: 1.0151 - acc: 0.6642137/250 [===============>..............] - ETA: 1s - loss: 1.0285 - acc: 0.665 - ETA: 1s - loss: 0.9890 - acc: 0.672 - ETA: 1s - loss: 1.0329 - acc: 0.664 - ETA: 1s - loss: 1.0130 - acc: 0.664 - ETA: 1s - loss: 1.0250 - acc: 0.667 - ETA: 1s - loss: 0.9984 - acc: 0.670 - ETA: 1s - loss: 1.0359 - acc: 0.663 - ETA: 1s - loss: 1.0142 - acc: 0.664 - ETA: 1s - loss: 1.0318 - acc: 0.663 - ETA: 1s - loss: 1.0002 - acc: 0.670 - ETA: 1s - loss: 1.0388 - acc: 0.66162/250 [==================>...........] - ETA: 1s - loss: 1.0354 - acc: 0.662 - ETA: 1s - loss: 1.0010 - acc: 0.670 - ETA: 1s - loss: 1.0340 - acc: 0.661 - ETA: 0s - loss: 1.0133 - acc: 0.664 - ETA: 1s - loss: 1.0445 - acc: 0.661 - ETA: 1s - loss: 1.0061 - acc: 0.669 - ETA: 1s - loss: 1.0401 - acc: 0.660 - ETA: 0s - loss: 1.0179 - acc: 0.663 - ETA: 0s - loss: 1.0404 - acc: 0.661 - ETA: 0s - loss: 1.0112 - acc: 0.667 - ETA: 1s - loss: 1.0333 - acc: 0.662 - ETA: 0s - loss: 1.0427 - acc: 0.6611198/250 [======================>.......] - ETA: 0s - loss: 1.0209 - acc: 0.663 - ETA: 0s - loss: 1.0161 - acc: 0.66207/250 [=======================>......]188/250 [=====================>........] - ETA: 0s - loss: 1.0262 - acc: 0.6621 - ETA: 0s - loss: 1.0341 - acc: 0.661 - ETA: 0s - loss: 1.0453 - acc: 0.661 - ETA: 0s - loss: 1.0184 - acc: 0.66215/250 [========================>.....] - ETA: 0s - loss: 1.0245 - acc: 0.6620196/250 [======================>.......] - ETA: 0s - loss: 1.0355 - acc: 0.662 - ETA: 0s - loss: 1.0399 - acc: 0.661 - ETA: 0s - loss: 1.0194 - acc: 0.666203/250 [=======================>......] - ETA: 0s - loss: 1.0334 - acc: 0.6622 - ETA: 0s - loss: 1.0263 - acc: 0.661 - ETA: 0s - loss: 1.0402 - acc: 0.661 - ETA: 0s - loss: 1.0182 - acc: 0.66 - ETA: 0s - loss: 1.0357 - acc: 0.6601 - ETA: 0s - loss: 1.0297 - acc: 0.660 - ETA: 0s - loss: 1.0370 - acc: 0.662 - ETA: 0s - loss: 1.0206 - acc: 0.66221/250 [=========================>....] - ETA: 0s - loss: 1.0406 - acc: 0.6586241/250 [===========================>..] - ETA: 0s - loss: 1.0341 - acc: 0.659 - ETA: 0s - loss: 1.0370 - acc: 0.661 - ETA: 0s - loss: 1.0213 - acc: 0.663230/250 [==========================>...] - 3s 12ms/step - loss: 1.0329 - acc: 0.6602\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0367 - acc: 0.661 - ETA: 0s - loss: 1.0191 - acc: 0.664 - ETA: 0s - loss: 1.0419 - acc: 0.658 - ETA: 0s - loss: 1.0406 - acc: 0.660 - ETA: 0s - loss: 1.0213 - acc: 0.664 - 3s 12ms/step - loss: 1.0409 - acc: 0.6610\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0433 - acc: 0.657 - 3s 12ms/step - loss: 1.0228 - acc: 0.6634\n",
      "250/250 [==============================] - 3s 12ms/step - loss: 1.0427 - acc: 0.6572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 0.8055 - acc: 0.720 - ETA: 2s - loss: 0.9486 - acc: 0.685 - ETA: 1s - loss: 1.0336 - acc: 0.6686 - ETA: 2s - loss: 1.1273 - acc: 0.740 - ETA: 1s - loss: 1.0423 - acc: 0.6667  1/250 [..............................]  1/250 [..............................] - ETA: 0s - loss: 1.2542 - acc: 0.6200 - ETA: 3s - loss: 0.9927 - acc: 0.600 - ETA: 1s - loss: 1.0987 - acc: 0.642 - ETA: 1s - loss: 1.0633 - acc: 0.65  9/250 [>.............................]  9/250 [>.............................] - ETA: 1s - loss: 1.0594 - acc: 0.6711 - ETA: 1s - loss: 1.0586 - acc: 0.653 - ETA: 1s - loss: 1.0369 - acc: 0.658 - ETA: 1s - loss: 1.0530 - acc: 0.653 - ETA: 1s - loss: 1.0516 - acc: 0.655 - ETA: 1s - loss: 1.0616 - acc: 0.645 - ETA: 1s - loss: 1.0244 - acc: 0.663 - ETA: 1s - loss: 1.0556 - acc: 0.65 - ETA: 1s - loss: 1.0500 - acc: 0.6536 - ETA: 1s - loss: 1.0191 - acc: 0.665 - ETA: 1s - loss: 1.0344 - acc: 0.662 - ETA: 1s - loss: 1.0331 - acc: 0.659 - ETA: 1s - loss: 1.0270 - acc: 0.655 - ETA: 1s - loss: 1.0139 - acc: 0.66 - ETA: 1s - loss: 1.0529 - acc: 0.6595 - ETA: 1s - loss: 1.0290 - acc: 0.65 - ETA: 1s - loss: 1.0281 - acc: 0.6633 - ETA: 1s - loss: 0.9982 - acc: 0.661 51/250 [=====>........................] 72/250 [=======>......................] - ETA: 1s - loss: 1.0301 - acc: 0.65 - ETA: 1s - loss: 1.0322 - acc: 0.6592 - ETA: 1s - loss: 1.0094 - acc: 0.662 - ETA: 1s - loss: 1.0458 - acc: 0.658 - ETA: 1s - loss: 1.0332 - acc: 0.65 - ETA: 1s - loss: 1.0225 - acc: 0.6672 - ETA: 1s - loss: 1.0232 - acc: 0.660 - ETA: 1s - loss: 1.0339 - acc: 0.659 - ETA: 1s - loss: 1.0340 - acc: 0.65 67/250 [=======>......................] 66/250 [======>.......................] - ETA: 1s - loss: 1.0220 - acc: 0.6684 - ETA: 1s - loss: 1.0415 - acc: 0.65 98/250 [==========>...................] - ETA: 1s - loss: 1.0313 - acc: 0.655 - ETA: 1s - loss: 1.0434 - acc: 0.656 - ETA: 1s - loss: 1.0343 - acc: 0.664 - ETA: 1s - loss: 1.0350 - acc: 0.658 - ETA: 0s - loss: 1.0235 - acc: 0.657 - ETA: 1s - loss: 1.0574 - acc: 0.652 - ETA: 1s - loss: 1.0338 - acc: 0.663 - ETA: 0s - loss: 1.0381 - acc: 0.658 - ETA: 0s - loss: 1.0210 - acc: 0.660 - ETA: 1s - loss: 1.0638 - acc: 0.650 - ETA: 1s - loss: 1.0444 - acc: 0.659 - ETA: 0s - loss: 1.0409 - acc: 0.658 - ETA: 0s - loss: 1.0287 - acc: 0.66 - ETA: 1s - loss: 1.0562 - acc: 0.6510 - ETA: 0s - loss: 1.0419 - acc: 0.658 - ETA: 0s - loss: 1.0525 - acc: 0.655 - ETA: 0s - loss: 1.0304 - acc: 0.659109/250 [============>.................] - ETA: 0s - loss: 1.0590 - acc: 0.6506 - ETA: 0s - loss: 1.0424 - acc: 0.658 - ETA: 0s - loss: 1.0556 - acc: 0.654 - ETA: 0s - loss: 1.0333 - acc: 0.656117/250 [=============>................] - ETA: 0s - loss: 1.0417 - acc: 0.6586 - ETA: 0s - loss: 1.0526 - acc: 0.650 - ETA: 0s - loss: 1.0505 - acc: 0.655 - ETA: 0s - loss: 1.0323 - acc: 0.65 - ETA: 0s - loss: 1.0505 - acc: 0.6524 - ETA: 0s - loss: 1.0423 - acc: 0.659 - ETA: 0s - loss: 1.0564 - acc: 0.654 - ETA: 0s - loss: 1.0329 - acc: 0.658 - ETA: 0s - loss: 1.0483 - acc: 0.653 - ETA: 0s - loss: 1.0399 - acc: 0.659 - ETA: 0s - loss: 1.0573 - acc: 0.655 - ETA: 0s - loss: 1.0354 - acc: 0.657 - ETA: 0s - loss: 1.0443 - acc: 0.655 - ETA: 0s - loss: 1.0416 - acc: 0.659 - ETA: 0s - loss: 1.0573 - acc: 0.656 - ETA: 0s - loss: 1.0336 - acc: 0.659 - ETA: 0s - loss: 1.0407 - acc: 0.656 - ETA: 0s - loss: 1.0372 - acc: 0.661 - ETA: 0s - loss: 1.0539 - acc: 0.657 - ETA: 0s - loss: 1.0385 - acc: 0.659 - ETA: 0s - loss: 1.0382 - acc: 0.657 - ETA: 0s - loss: 1.0420 - acc: 0.660 - ETA: 0s - loss: 1.0547 - acc: 0.658 - ETA: 0s - loss: 1.0378 - acc: 0.660 - ETA: 0s - loss: 1.0392 - acc: 0.656 - ETA: 0s - loss: 1.0378 - acc: 0.659 - ETA: 0s - loss: 1.0509 - acc: 0.660 - ETA: 0s - loss: 1.0390 - acc: 0.659 - ETA: 0s - loss: 1.0432 - acc: 0.656 - ETA: 0s - loss: 1.0373 - acc: 0.660 - ETA: 0s - loss: 1.0555 - acc: 0.659 - ETA: 0s - loss: 1.0401 - acc: 0.659 - ETA: 0s - loss: 1.0470 - acc: 0.654 - ETA: 0s - loss: 1.0414 - acc: 0.658 - ETA: 0s - loss: 1.0476 - acc: 0.660 - ETA: 0s - loss: 1.0435 - acc: 0.657 - ETA: 0s - loss: 1.0475 - acc: 0.653 - ETA: 0s - loss: 1.0476 - acc: 0.65 - ETA: 0s - loss: 1.0537 - acc: 0.6596 - ETA: 0s - loss: 1.0422 - acc: 0.658 - ETA: 0s - loss: 1.0457 - acc: 0.654 - ETA: 0s - loss: 1.0486 - acc: 0.65 - ETA: 0s - loss: 1.0482 - acc: 0.6602 - ETA: 0s - loss: 1.0433 - acc: 0.658 - ETA: 0s - loss: 1.0477 - acc: 0.654 - ETA: 0s - loss: 1.0482 - acc: 0.65 - ETA: 0s - loss: 1.0432 - acc: 0.6588 - ETA: 0s - loss: 1.0517 - acc: 0.659 - ETA: 0s - loss: 1.0494 - acc: 0.653 - ETA: 0s - loss: 1.0495 - acc: 0.65246/250 [============================>.]226/250 [==========================>...] - ETA: 0s - loss: 1.0469 - acc: 0.6589 - ETA: 0s - loss: 1.0563 - acc: 0.657 - 2s 7ms/step - loss: 1.0485 - acc: 0.6586\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.0478 - acc: 0.654 - ETA: 0s - loss: 1.0475 - acc: 0.656 - ETA: 0s - loss: 1.0552 - acc: 0.658 - ETA: 0s - loss: 1.0490 - acc: 0.653 - ETA: 0s - loss: 1.0475 - acc: 0.658 - ETA: 0s - loss: 1.0534 - acc: 0.658239/250 [===========================>..] - ETA: 0s - loss: 1.0480 - acc: 0.6531241/250 [===========================>..] - ETA: 0s - loss: 1.0392 - acc: 0.6612 - 2s 6ms/step - loss: 1.0579 - acc: 0.6574\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.0442 - acc: 0.653 - ETA: 0s - loss: 1.0396 - acc: 0.660250/250 [==============================] - 2s 6ms/step - loss: 1.0425 - acc: 0.6541\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.0391 - acc: 0.6606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 2s - loss: 1.1649 - acc: 0.600 - ETA: 2s - loss: 1.0086 - acc: 0.685 - ETA: 1s - loss: 1.0456 - acc: 0.645 - ETA: 1s - loss: 1.0116 - acc: 0.6571 - ETA: 1s - loss: 0.8748 - acc: 0.6600 28/250 [==>...........................] - ETA: 1s - loss: 0.9903 - acc: 0.664  1/250 [..............................]  9/250 [>.............................] - ETA: 1s - loss: 1.0165 - acc: 0.6667 - ETA: 2s - loss: 0.9772 - acc: 0.720 36/250 [===>..........................]  1/250 [..............................] - ETA: 1s - loss: 0.9948 - acc: 0.6661 - ETA: 3s - loss: 0.9553 - acc: 0.58 17/250 [=>............................] - ETA: 1s - loss: 1.0141 - acc: 0.6467 - ETA: 1s - loss: 1.0703 - acc: 0.6400 - ETA: 1s - loss: 1.0099 - acc: 0.662 - ETA: 1s - loss: 1.0824 - acc: 0.63 - ETA: 1s - loss: 1.0571 - acc: 0.657 52/250 [=====>........................] - ETA: 1s - loss: 1.0146 - acc: 0.665 - ETA: 1s - loss: 1.0177 - acc: 0.652 33/250 [==>...........................] - ETA: 1s - loss: 1.0426 - acc: 0.652 - ETA: 1s - loss: 1.0303 - acc: 0.6551 - ETA: 1s - loss: 1.0688 - acc: 0.636 - ETA: 1s - loss: 1.0306 - acc: 0.642 - ETA: 1s - loss: 1.0413 - acc: 0.653 - ETA: 1s - loss: 1.0389 - acc: 0.6543 34/250 [===>..........................] - ETA: 1s - loss: 1.0418 - acc: 0.644 - ETA: 1s - loss: 1.0029 - acc: 0.648 - ETA: 1s - loss: 1.0391 - acc: 0.65 - ETA: 1s - loss: 1.0448 - acc: 0.6467 - ETA: 1s - loss: 1.0576 - acc: 0.650 - ETA: 1s - loss: 1.0088 - acc: 0.652 - ETA: 1s - loss: 1.0333 - acc: 0.65 88/250 [=========>....................] - ETA: 1s - loss: 1.0576 - acc: 0.653 - ETA: 1s - loss: 1.0076 - acc: 0.654 - ETA: 1s - loss: 1.0301 - acc: 0.654 58/250 [=====>........................] - ETA: 1s - loss: 1.0336 - acc: 0.6486 - ETA: 1s - loss: 1.0603 - acc: 0.651 - ETA: 1s - loss: 1.0219 - acc: 0.652 - ETA: 1s - loss: 1.0413 - acc: 0.650 - ETA: 0s - loss: 1.0562 - acc: 0.654 - ETA: 1s - loss: 1.0334 - acc: 0.6499 67/250 [=======>......................] - ETA: 1s - loss: 1.0249 - acc: 0.654 - ETA: 1s - loss: 1.0462 - acc: 0.651 - ETA: 0s - loss: 1.0582 - acc: 0.652 - ETA: 1s - loss: 1.0343 - acc: 0.654 - ETA: 1s - loss: 1.0413 - acc: 0.645 - ETA: 1s - loss: 1.0405 - acc: 0.653 84/250 [=========>....................] - ETA: 1s - loss: 1.0266 - acc: 0.6548 - ETA: 0s - loss: 1.0671 - acc: 0.649 - ETA: 1s - loss: 1.0443 - acc: 0.646 - ETA: 0s - loss: 1.0393 - acc: 0.650 - ETA: 0s - loss: 1.0611 - acc: 0.649 - ETA: 1s - loss: 1.0378 - acc: 0.653 - ETA: 1s - loss: 1.0444 - acc: 0.645 - ETA: 0s - loss: 1.0388 - acc: 0.65 - ETA: 0s - loss: 1.0580 - acc: 0.6496 - ETA: 0s - loss: 1.0419 - acc: 0.653 - ETA: 0s - loss: 1.0411 - acc: 0.645 - ETA: 0s - loss: 1.0347 - acc: 0.653 - ETA: 0s - loss: 1.0524 - acc: 0.649 - ETA: 0s - loss: 1.0404 - acc: 0.654 - ETA: 0s - loss: 1.0425 - acc: 0.64 - ETA: 0s - loss: 1.0319 - acc: 0.6546153/250 [=================>............]117/250 [=============>................] - ETA: 0s - loss: 1.0389 - acc: 0.653 - ETA: 0s - loss: 1.0418 - acc: 0.645 - ETA: 0s - loss: 1.0323 - acc: 0.654162/250 [==================>...........] - ETA: 0s - loss: 1.0359 - acc: 0.6536 - ETA: 0s - loss: 1.0525 - acc: 0.651 - ETA: 0s - loss: 1.0379 - acc: 0.647 - ETA: 0s - loss: 1.0355 - acc: 0.653 - ETA: 0s - loss: 1.0439 - acc: 0.654 - ETA: 0s - loss: 1.0380 - acc: 0.653 - ETA: 0s - loss: 1.0340 - acc: 0.64 - ETA: 0s - loss: 1.0350 - acc: 0.652 - ETA: 0s - loss: 1.0400 - acc: 0.6563 - ETA: 0s - loss: 1.0415 - acc: 0.652 - ETA: 0s - loss: 1.0314 - acc: 0.65187/250 [=====================>........]150/250 [=================>............] - ETA: 0s - loss: 1.0437 - acc: 0.6517 - ETA: 0s - loss: 1.0400 - acc: 0.651 - ETA: 0s - loss: 1.0346 - acc: 0.64 - ETA: 0s - loss: 1.0402 - acc: 0.651 - ETA: 0s - loss: 1.0376 - acc: 0.6578 - ETA: 0s - loss: 1.0411 - acc: 0.652 - ETA: 0s - loss: 1.0345 - acc: 0.64 - ETA: 0s - loss: 1.0434 - acc: 0.6499167/250 [===================>..........] - ETA: 0s - loss: 1.0448 - acc: 0.6493 - ETA: 0s - loss: 1.0386 - acc: 0.657 - ETA: 0s - loss: 1.0392 - acc: 0.649 - ETA: 0s - loss: 1.0458 - acc: 0.64211/250 [========================>.....] - ETA: 0s - loss: 1.0368 - acc: 0.6578175/250 [====================>.........] - ETA: 0s - loss: 1.0490 - acc: 0.648 - ETA: 0s - loss: 1.0385 - acc: 0.64183/250 [====================>.........]192/250 [======================>.......] - ETA: 0s - loss: 1.0504 - acc: 0.6488 - ETA: 0s - loss: 1.0474 - acc: 0.648 - ETA: 0s - loss: 1.0338 - acc: 0.658 - ETA: 0s - loss: 1.0362 - acc: 0.650191/250 [=====================>........ - ETA: 0s - loss: 1.0313 - acc: 0.6587200/250 [=======================>......] - ETA: 0s - loss: 1.0494 - acc: 0.649 - ETA: 0s - loss: 1.0366 - acc: 0.650 - ETA: 0s - loss: 1.0484 - acc: 0.650199/250 [======================>.......] - ETA: 0s - loss: 1.0435 - acc: 0.6516 - ETA: 0s - loss: 1.0370 - acc: 0.657 - ETA: 0s - loss: 1.0394 - acc: 0.65 - ETA: 0s - loss: 1.0466 - acc: 0.6495 - ETA: 0s - loss: 1.0457 - acc: 0.651208/250 [=======================>......] - ETA: 0s - loss: 1.0397 - acc: 0.6569 - ETA: 0s - loss: 1.0360 - acc: 0.652 - 2s 7ms/step - loss: 1.0368 - acc: 0.6578\n",
      "217/250 [=========================>....] - ETA: 0s - loss: 1.0450 - acc: 0.6523 - ETA: 0s - loss: 1.0441 - acc: 0.650 - ETA: 0s - loss: 1.0327 - acc: 0.6540"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/250 [===========================>..] - ETA: 0s - loss: 1.0416 - acc: 0.650 - ETA: 0s - loss: 1.0434 - acc: 0.653 - ETA: 0s - loss: 1.0331 - acc: 0.654 - ETA: 0s - loss: 1.0413 - acc: 0.651 - ETA: 0s - loss: 1.0420 - acc: 0.654 - ETA: 0s - loss: 1.0324 - acc: 0.655250/250 [==============================] - ETA: 0s - loss: 1.0421 - acc: 0.6540 - 2s 6ms/step - loss: 1.0407 - acc: 0.6526\n",
      "244/250 [============================>.] - ETA: 0s - loss: 1.0312 - acc: 0.65250/250 [==============================]250/250 [==============================] - 2s 6ms/step - loss: 1.0319 - acc: 0.6568\n",
      " - 2s 6ms/step - loss: 1.0423 - acc: 0.6550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n",
      "/home/mnetlab/Jeff/model/operation.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  model_m.layers[layer].get_weights()))\n"
     ]
    }
   ],
   "source": [
    "# Adjust parameters of the model\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "# Define the method for preprocessing\n",
    "augment = ImageDataGenerator(preprocessing_function=preprocessing_for_training)\n",
    "\n",
    "# bind zmq socket port\n",
    "socket = zmq_bind()\n",
    "\n",
    " # Read detailed settings from json file\n",
    "detailed_setting = read_setting()\n",
    "training_info = detailed_setting[\"training_info\"]\n",
    "\n",
    "while True:\n",
    "    while True:\n",
    "        # Recieve message from server\n",
    "        message = socket.recv()\n",
    "        ms = str(message, encoding = \"utf-8\").split(\":\")\n",
    "        r = int(ms[0])\n",
    "        device = int(ms[1])\n",
    "        print(\"(Received) round: %s, device: %s\" % (message, device))\n",
    "        \n",
    "        # Start process for the device\n",
    "        locals()['p_{}'.format(device)] = mp.Process(target=model_training, args=(r, device))\n",
    "        locals()['p_{}'.format(device)].start()\n",
    "        \n",
    "        # Send message back to master\n",
    "        # Tell master that the process (device) starts local training\n",
    "        socket.send(b\"ok\")\n",
    "        if(device == (training_info['num_device']-1)): break\n",
    "    \n",
    "    # Wait all processes done the jobs (local training)\n",
    "    for device in range(training_info['num_device']):\n",
    "        locals()['p_{}'.format(device)].join()\n",
    "\n",
    "    # Tell server that the 'r' round finishs\n",
    "    message = socket.recv()\n",
    "    socket.send(b\"finish\")\n",
    "    if(r == (training_info['num_round']-1)): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pp_fl",
   "language": "python",
   "name": "pp_fl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
